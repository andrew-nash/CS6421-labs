{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7H3yTncQfoym"
      },
      "source": [
        "<a target=\"_blank\" href=\"https://colab.research.google.com/github/andrew-nash/CS6421-labs/blob/main/CS6421_Assignment2.ipynb\">\n",
        "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
        "</a>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "h1CiDh7CfqON"
      },
      "outputs": [],
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wandb -qU\n",
        "!wget https://github.com/gmprovan/CS6421-Assignment1/raw/master/train.zip\n",
        "!wget https://github.com/gmprovan/CS6421-Assignment1/raw/master/test.zip\n",
        "!wget https://github.com/gmprovan/CS6421-Assignment1/raw/master/train_cleaned.zip\n",
        "!unzip train.zip\n",
        "!unzip test.zip\n",
        "!unzip train_cleaned.zip"
      ],
      "metadata": {
        "id": "ptuuMfe5cRCO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext tensorboard\n",
        "%tensorboard --logdir tboard"
      ],
      "metadata": {
        "id": "3PpszFCP14J8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup"
      ],
      "metadata": {
        "id": "FebDKAJxxxTd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cs={\n",
        "    \"task1\": {\n",
        "        \"_descrip\" : \"\",\n",
        "        \"permitted_layers\" : [\"InputLayer\",\"Dense\",\"BatchNormalization\",\"Reshape\",\"Flatten\", \"Dropout\", \"Conv2D\", \"MaxPooling2D\", \"MaxPooling1D\", \"UpSampling2D\"],\n",
        "        \"optimizers\" : [\"SGD\", \"Adam\", \"Nadam\",\"Adadelta\",\"RMSprop\"],\n",
        "        \"act_funcs\" : [\"relu\",\"leaky_relu\",\"sigmoid\",\"elu\",\"linear\"],\n",
        "        \"min_lr\" :  0.000001,\n",
        "        \"max_lr\" :  0.1,\n",
        "        \"layer_limits\" : {\n",
        "            \"Dense\" : 30\n",
        "        },\n",
        "        \"layer_mins\" : {\n",
        "        },\n",
        "        \"max_epochs\" : 100,\n",
        "        \"total_param_limit\" :100_000_000,\n",
        "        \"kernel_regularizer\": True,\n",
        "        \"bias_regularizer\": True,\n",
        "        \"activity_regularizer\": True\n",
        "    },\n",
        "    \"task2\": {\n",
        "        \"_descrip\" : \"\",\n",
        "        \"permitted_layers\" : [\"InputLayer\",\"Dense\",\"BatchNormalization\",\"Reshape\",\"Flatten\", \"Dropout\", \"Conv2D\", \"MaxPooling2D\", \"MaxPooling1D\", \"UpSampling2D\"],\n",
        "        \"optimizers\" : [\"SGD\", \"Adam\", \"Nadam\",\"Adadelta\",\"RMSprop\"],\n",
        "        \"act_funcs\" : [\"relu\",\"leaky_relu\",\"sigmoid\",\"elu\",\"linear\"],\n",
        "        \"min_lr\" :  0.000001,\n",
        "        \"max_lr\" :  0.1,\n",
        "        \"layer_limits\" : {\n",
        "            \"Dense\" : 30\n",
        "        },\n",
        "        \"layer_mins\" : {\n",
        "        },\n",
        "        \"max_epochs\" : 100,\n",
        "        \"total_param_limit\" :100_000_000,\n",
        "        \"kernel_regularizer\": True,\n",
        "        \"bias_regularizer\": True,\n",
        "        \"activity_regularizer\": True\n",
        "    },\n",
        "    \"task3\": {\n",
        "        \"_descrip\" : \"\",\n",
        "        \"permitted_layers\" : [\"InputLayer\",\"Dense\",\"BatchNormalization\",\"Reshape\",\"Flatten\", \"Dropout\", \"Conv2D\", \"MaxPooling2D\", \"MaxPooling1D\", \"UpSampling2D\"],\n",
        "        \"optimizers\" : [\"SGD\", \"Adam\", \"Nadam\",\"Adadelta\",\"RMSprop\"],\n",
        "        \"act_funcs\" : [\"relu\",\"leaky_relu\",\"sigmoid\",\"elu\",\"linear\"],\n",
        "        \"min_lr\" :  0.000001,\n",
        "        \"max_lr\" :  0.1,\n",
        "        \"layer_limits\" : {\n",
        "            \"Dense\" : 30\n",
        "        },\n",
        "        \"layer_mins\" : {\n",
        "        },\n",
        "        \"max_epochs\" : 100,\n",
        "        \"total_param_limit\" :100_000_000,\n",
        "        \"kernel_regularizer\": True,\n",
        "        \"bias_regularizer\": True,\n",
        "        \"activity_regularizer\": True\n",
        "    },\n",
        "}"
      ],
      "metadata": {
        "id": "OGblrq7hgVLn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import json\n",
        "from google.colab import files\n",
        "import numpy as np\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import wandb\n",
        "from keras.preprocessing.image import load_img, array_to_img, img_to_array\n",
        "from sklearn.model_selection import train_test_split\n",
        "from pathlib import Path\n",
        "from wandb.keras import WandbMetricsLogger, WandbModelCheckpoint\n",
        "def rmse_loss(preds, real):\n",
        "  return tf.reduce_mean(tf.square(tf.subtract(preds, real)))\n",
        "class Assignment2:\n",
        "    def __init__(self):\n",
        "        self.constraints = cs\n",
        "\n",
        "    def sample_constraint(self, mod, task, epoch):\n",
        "        task_constraints = self.constraints[task]\n",
        "        permitted_layers = task_constraints['permitted_layers']\n",
        "        layer_limits = task_constraints['layer_limits']\n",
        "        layer_mins = task_constraints['layer_mins']\n",
        "        total_param_limit = task_constraints['total_param_limit']\n",
        "        max_epochs= task_constraints['max_epochs']\n",
        "        kernel_regularizer = task_constraints['kernel_regularizer']\n",
        "        bias_regularizer = task_constraints['bias_regularizer']\n",
        "        activity_regularizer = task_constraints['activity_regularizer']\n",
        "        regularizers = [ 'kernel_regularizer', 'bias_regularizer','activity_regularizer' ]\n",
        "        regularizer_allowed = [kernel_regularizer, bias_regularizer, activity_regularizer]\n",
        "        act_funcs = task_constraints['act_funcs']\n",
        "\n",
        "        min_lr = task_constraints['min_lr']\n",
        "        max_lr = task_constraints['max_lr']\n",
        "        permitted_opts =  task_constraints['optimizers']\n",
        "\n",
        "        # both trainable and untrainable\n",
        "        if mod.count_params()>total_param_limit:\n",
        "            return False, \"Too many parameters\"\n",
        "        layer_counts = {}\n",
        "        config = mod.get_config()\n",
        "        comp_config = mod.optimizer.get_config()\n",
        "        if epoch>max_epochs:\n",
        "            return (False, f\"Too many trianing epochs, limit is \\'{max_epochs}\\'\")\n",
        "\n",
        "        if comp_config['name'] not in permitted_opts:\n",
        "            return (False, f\"Unpermitted Optimizer \\'{comp_config['name']}\\'\")\n",
        "        if comp_config['learning_rate'] < min_lr and not abs(comp_config['learning_rate'] - min_lr)<1e-5:\n",
        "            return (False, f\"Learning rate {comp_config['learning_rate']} lower than specified minimum \\'{min_lr}\\'\")\n",
        "        if comp_config['learning_rate'] > max_lr and not abs(comp_config['learning_rate'] - max_lr)<1e-5:\n",
        "            return (False, f\"Learning rate {comp_config['learning_rate']} higher than specified max \\'{max_lr}\\'\")\n",
        "\n",
        "        for layer in config['layers']:\n",
        "            l_type = layer['class_name']\n",
        "            if l_type not in permitted_layers:\n",
        "                return (False, f\"Unpermitted layer type \\'{l_type}\\'\")\n",
        "            if l_type not in layer_counts: layer_counts[l_type]=0\n",
        "            layer_counts[l_type]+=1\n",
        "            if l_type in layer_limits and layer_counts[l_type]>layer_limits[l_type]:\n",
        "                return (False, f\"Limit of \\'{l_type}\\' layers exceeded\")\n",
        "            if 'activation' in layer['config'] and layer['config']['activation'] not in act_funcs:\n",
        "              return (False, f\"Unpermitted activation function \\'{layer['config']['activation']}\\'\")\n",
        "\n",
        "            for regularizer, allowed in zip(regularizers,regularizer_allowed):\n",
        "                if regularizer in layer['config']:\n",
        "                    if layer['config'][regularizer]!=None and not allowed:\n",
        "                        return (False, f\"Unpermitted regularizer: {regularizer}\")\n",
        "        for l_type in layer_mins:\n",
        "            if layer_counts[l_type]<layer_mins[l_type]:\n",
        "                return (False, f\"Need to include a minmum of \\'{layer_mins[l_type]}\\' {l_type} layers\")\n",
        "        return True, \"Model satisifes constraints\"\n",
        "    def getTask3SampleImage(self, tpe=\"train\"):\n",
        "      X = []\n",
        "      Y = []\n",
        "\n",
        "      for img in os.listdir(\"train\"):\n",
        "          img = load_img(f\"train/{img}\", grayscale=True,target_size=(420,540))\n",
        "          img = img_to_array(img).astype('float32')/255.\n",
        "          X.append(img)\n",
        "\n",
        "      for img in os.listdir(\"train_cleaned\"):\n",
        "          img = load_img(f\"train_cleaned/{img}\", grayscale=True,target_size=(420,540))\n",
        "          img = img_to_array(img).astype('float32')/255.\n",
        "          Y.append(img)\n",
        "\n",
        "\n",
        "      X = np.array(X)\n",
        "      Y = np.array(Y)\n",
        "\n",
        "      X_train, X_valid, y_train, y_valid = train_test_split(X, Y, test_size=0.1, random_state=111)\n",
        "\n",
        "      if tpe=='train':\n",
        "          idx=np.random.choice(list(range(len(X_train))))\n",
        "          return X_train[idx],y_train[idx]\n",
        "      else:\n",
        "        idx=np.random.choice(list(range(len(X_valid))))\n",
        "        return X_valid[idx],y_valid[idx]\n",
        "\n",
        "    def trainTaskOne(self, model, run_name,  batch_size):\n",
        "\n",
        "        (x_train, _), (x_test, _) = mnist.load_data()\n",
        "        np.random.seed(42)\n",
        "        np.random.shuffle(x_train)\n",
        "        split = int(0.75*len(x_train))\n",
        "        x_valid = x_train[split:,:,:]\n",
        "        x_train = x_train[:split,:,:]\n",
        "        x_train = x_train.astype('float32') / 255.\n",
        "        x_train = np.reshape(x_train, (len(x_train), 28, 28, 1))\n",
        "        x_valid = x_valid.astype('float32') / 255.\n",
        "        x_valid = np.reshape(x_valid, (len(x_valid), 28, 28, 1))\n",
        "\n",
        "        task = \"task1\"\n",
        "        project_name = task\n",
        "        run_name = run_name.strip()\n",
        "        wandb.init(\n",
        "        project= project_name,\n",
        "        name = run_name)\n",
        "        if not run_name.replace('_','').isalnum():\n",
        "          raise ValueError(\"Run name can only contain alphanumeric caracters and _\")\n",
        "        #model.compile(optimizer=optimizer, loss=loss_function)\n",
        "        callback_ckpt = tf.keras.callbacks.ModelCheckpoint(filepath=task+'/'+task+\"_{epoch:03d}\",\n",
        "                                                 save_weights_only=True,\n",
        "                                                 save_best_only = False,\n",
        "                                                 monitor=\"val_loss\",\n",
        "                                                 verbose=1)\n",
        "        check, msg = self.sample_constraint(model, task, 50)\n",
        "        if not check:\n",
        "          raise ValueError(msg)\n",
        "        filename=f'{task}/history_log.csv'\n",
        "        if not os.path.exists(task):\n",
        "            os.makedirs(task)\n",
        "\n",
        "        callback_hst = AssignmentSubmissionCallback(filename, x_train,x_train)\n",
        "        tensorboard_callback = tf.keras.callbacks.TensorBoard(f\"./tboard/{project_name}/{run_name}\", histogram_freq=1)\n",
        "        model.fit(x_train,\n",
        "                x_train,\n",
        "                batch_size=batch_size,\n",
        "                epochs=50,\n",
        "                validation_data=(x_valid, x_valid),\n",
        "                  callbacks=[callback_ckpt,callback_hst,tensorboard_callback,WandbMetricsLogger()], verbose=0)\n",
        "        wandb.finish()\n",
        "        model.save(f\"./{task}/final_model\")\n",
        "        os.system(f\"tar -cf {task}_{run_name}.tar.gz ./{task}\")\n",
        "        files.download(f'{task}_{run_name}.tar.gz')\n",
        "\n",
        "    def trainTaskTwo(self, model, run_name,  batch_size):\n",
        "        (x_train, _), (x_test, _) = mnist.load_data()\n",
        "        np.random.seed(42)\n",
        "        np.random.shuffle(x_train)\n",
        "        noise_factor = 0.5\n",
        "        x_train_noisy = x_train + noise_factor + np.random.normal(loc=0.0, scale=1.0, size=x_train.shape)\n",
        "        x_train_noisy = np.clip(x_train_noisy, 0., 1.)\n",
        "        split = int(0.75*len(x_train))\n",
        "        x_valid = x_train[split:,:,:]\n",
        "        x_train = x_train[:split,:,:]\n",
        "        x_train = x_train.astype('float32') / 255.\n",
        "        x_train = np.reshape(x_train, (len(x_train), 28, 28, 1))\n",
        "        x_valid = x_valid.astype('float32') / 255.\n",
        "        x_valid = np.reshape(x_valid, (len(x_valid), 28, 28, 1))\n",
        "\n",
        "        task = \"task2\"\n",
        "        project_name = task\n",
        "        run_name = run_name.strip()\n",
        "        wandb.init(\n",
        "        project= project_name,\n",
        "        name = run_name)\n",
        "        if not run_name.replace('_','').isalnum():\n",
        "          raise ValueError(\"Run name can only contain alphanumeric caracters and _\")\n",
        "        #model.compile(optimizer=optimizer, loss=loss_function)\n",
        "        callback_ckpt = tf.keras.callbacks.ModelCheckpoint(filepath=task+'/'+task+\"_{epoch:03d}\",\n",
        "                                                 save_weights_only=True,\n",
        "                                                 save_best_only = False,\n",
        "                                                 monitor=\"val_loss\",\n",
        "                                                 verbose=1)\n",
        "        check, msg = self.sample_constraint(model, task, 50)\n",
        "        if not check:\n",
        "          raise ValueError(msg)\n",
        "        filename=f'{task}/history_log.csv'\n",
        "        if not os.path.exists(task):\n",
        "            os.makedirs(task)\n",
        "\n",
        "        callback_hst = AssignmentSubmissionCallback(filename, x_train,x_train)\n",
        "        tensorboard_callback = tf.keras.callbacks.TensorBoard(f\"./tboard/{project_name}/{run_name}\", histogram_freq=1)\n",
        "        model.fit(x_train,\n",
        "                x_train,\n",
        "                batch_size=batch_size,\n",
        "                epochs=50,\n",
        "                validation_data=(x_valid, x_valid),\n",
        "                  callbacks=[callback_ckpt,callback_hst,tensorboard_callback,WandbMetricsLogger()], verbose=0)\n",
        "        wandb.finish()\n",
        "        model.save(f\"./{task}/final_model\")\n",
        "        os.system(f\"tar -cf {task}_{run_name}.tar.gz ./{task}\")\n",
        "        files.download(f'{task}_{run_name}.tar.gz')\n",
        "\n",
        "    def trainTaskThree(self, model, run_name,  batch_size):\n",
        "\n",
        "        X = []\n",
        "        Y = []\n",
        "\n",
        "        for img in os.listdir(\"train\"):\n",
        "            img = load_img(f\"train/{img}\", grayscale=True,target_size=(420,540))\n",
        "            img = img_to_array(img).astype('float32')/255.\n",
        "            X.append(img)\n",
        "\n",
        "        for img in os.listdir(\"train_cleaned\"):\n",
        "            img = load_img(f\"train_cleaned/{img}\", grayscale=True,target_size=(420,540))\n",
        "            img = img_to_array(img).astype('float32')/255.\n",
        "            Y.append(img)\n",
        "\n",
        "\n",
        "        X = np.array(X)\n",
        "        Y = np.array(Y)\n",
        "\n",
        "        X_train, X_valid, y_train, y_valid = train_test_split(X, Y, test_size=0.1, random_state=111)\n",
        "\n",
        "\n",
        "        task = \"task3\"\n",
        "        project_name = task\n",
        "        run_name = run_name.strip()\n",
        "        wandb.init(\n",
        "        project= project_name,\n",
        "        name = run_name)\n",
        "        if not run_name.replace('_','').isalnum():\n",
        "          raise ValueError(\"Run name can only contain alphanumeric caracters and _\")\n",
        "        #model.compile(optimizer=optimizer, loss=loss_function)\n",
        "        callback_ckpt = tf.keras.callbacks.ModelCheckpoint(filepath=task+'/'+task+\"_{epoch:03d}\",\n",
        "                                                 save_weights_only=True,\n",
        "                                                 save_best_only = False,\n",
        "                                                 monitor=\"val_loss\",\n",
        "                                                 verbose=1)\n",
        "        check, msg = self.sample_constraint(model, task, 50)\n",
        "        if not check:\n",
        "          raise ValueError(msg)\n",
        "        filename=f'{task}/history_log.csv'\n",
        "        if not os.path.exists(task):\n",
        "            os.makedirs(task)\n",
        "\n",
        "        callback_hst = AssignmentSubmissionCallback(filename, x_train,x_train)\n",
        "        tensorboard_callback = tf.keras.callbacks.TensorBoard(f\"./tboard/{project_name}/{run_name}\", histogram_freq=1)\n",
        "        model.fit(X_train,\n",
        "                y_train,\n",
        "                batch_size=batch_size,\n",
        "                epochs=50,\n",
        "                validation_data=(X_valid, y_valid),\n",
        "                  callbacks=[callback_ckpt,callback_hst,tensorboard_callback,WandbMetricsLogger()], verbose=0)\n",
        "        wandb.finish()\n",
        "        model.save(f\"./{task}/final_model\")\n",
        "        os.system(f\"tar -cf {task}_{run_name}.tar.gz ./{task}\")\n",
        "        files.download(f'{task}_{run_name}.tar.gz')\n",
        "\n",
        "    def setup(self):\n",
        "      wandb.login()\n",
        "\n",
        "    def make_sub(self, task, chosen_run1, chosen_run2, just):\n",
        "      while not os.path.exists(task+'_'+chosen_run1+'.tar.gz'):\n",
        "          print(f\"ERROR: Missing the run! Find the file {task+'_'+chosen_run1}.tar.gz in your downloads, and upload it in the left hand pane and click enter.\")\n",
        "          input()\n",
        "      while not os.path.exists(task+'_'+chosen_run2+'.tar.gz'):\n",
        "          print(f\"ERROR: Missing the run! Find the file {task+'_'+chosen_run2}.tar.gz in your downloads, and upload it in the left hand pane and click enter.\")\n",
        "          input()\n",
        "\n",
        "      f=open(f\"./explain.txt\",\"w\")\n",
        "      f.write(just)\n",
        "      f.close()\n",
        "      os.system(f\"tar -cf {task}.tar.gz {task}_{chosen_run1}.tar.gz {task}_{chosen_run2}.tar.gz explain.txt\")\n",
        "      files.download(f'{task}.tar.gz')\n",
        "\n",
        "    def make_submission_one(self, chosen_run1, chosen_run2, just):\n",
        "      self.make_sub(\"task1\", chosen_run1, chosen_run2, just)\n",
        "    def make_submission_two(self, chosen_run1, chosen_run2, just):\n",
        "      self.make_sub(\"task2\", chosen_run1, chosen_run2, just)\n",
        "    def make_submission_three(self, chosen_run1, chosen_run2, chosen_run3, just):\n",
        "      task='task3'\n",
        "      while not os.path.exists(task+'_'+chosen_run1+'.tar.gz'):\n",
        "          print(f\"ERROR: Missing the run! Find the file {task+'_'+chosen_run1}.tar.gz in your downloads, and upload it in the left hand pane and click enter.\")\n",
        "          input()\n",
        "      while not os.path.exists(task+'_'+chosen_run2+'.tar.gz'):\n",
        "          print(f\"ERROR: Missing the run! Find the file {task+'_'+chosen_run2}.tar.gz in your downloads, and upload it in the left hand pane and click enter.\")\n",
        "          input()\n",
        "      while not os.path.exists(task+'_'+chosen_run3+'.tar.gz'):\n",
        "          print(f\"ERROR: Missing the run! Find the file {task+'_'+chosen_run3}.tar.gz in your downloads, and upload it in the left hand pane and click enter.\")\n",
        "          input()\n",
        "\n",
        "      f=open(f\"./explain.txt\",\"w\")\n",
        "      f.write(just)\n",
        "      f.close()\n",
        "      os.system(f\"tar -cf {task}.tar.gz {task}_{chosen_run1}.tar.gz {task}_{chosen_run2}.tar.gz {task}_{chosen_run3}.tar.gz explain.txt\")\n",
        "      files.download(f'{task}.tar.gz')\n",
        "\n",
        "\n",
        "\n",
        "class AssignmentSubmissionCallback(tf.keras.callbacks.Callback):\n",
        "   def __init__(self, filename, x_train, y_train):\n",
        "        super().__init__()\n",
        "        self.filename=filename\n",
        "        self.x_train = x_train\n",
        "        self.y_train = y_train\n",
        "        f = open(self.filename, \"w\")\n",
        "        f.write(\"epoch,train_loss,test_loss\\n\")\n",
        "        f.close()\n",
        "   def on_epoch_end(self, epoch, logs=None):\n",
        "      train_loss = self.model.evaluate(self.x_train, self.y_train, verbose=0)\n",
        "      if isinstance(train_loss, list): train_loss=train_loss[0]\n",
        "      test_loss = logs['val_loss']\n",
        "      f = open(self.filename, \"a\")\n",
        "      f.write(f\"{epoch},{train_loss},{test_loss}\\n\")\n",
        "      f.close()\n",
        "\n"
      ],
      "metadata": {
        "id": "Tsvn_WBKgX8v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# First steps"
      ],
      "metadata": {
        "id": "v4nvplttxzej"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "A = Assignment2()\n",
        "A.setup()"
      ],
      "metadata": {
        "id": "bO4_H7FOmMc_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pp-UaomMQJNo"
      },
      "source": [
        "# CS6421 Assignment 2: Autoencoders\n",
        "\n",
        " You will be given a tutorial introduction to the deep autoencoder, and will then need to use this model to solve real-world problems focusing on image denoising.\n",
        "\n",
        "\n",
        "Please submit your work ideally in a clear Jupyter notebook, highlighting the code that you have written. Present the comparisons and explanations of different model performance results in specific detail."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wLASBMNLsUe6"
      },
      "source": [
        "For each assignment, the following text needs to be agreed to:\n",
        "\n",
        " By submitting this exam, I declare\n",
        "\n",
        "(1) that all work of it is my own;\n",
        "\n",
        "(2) that I did not seek whole or partial solutions for any part of my submission from others; and\n",
        "\n",
        "(3) that I did not and will not discuss, exchange, share, or publish complete or partial solutions for this exam or any part of it."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NBhbug0Qgmuu"
      },
      "source": [
        "## Introduction: Basic Autoencoder\n",
        "In this assignment, we will create a **simple autoencoder** model using the [TensorFlow subclassing API](https://www.tensorflow.org/guide/keras#model_subclassing). We start with the popular [MNIST dataset](http://yann.lecun.com/exdb/mnist/) (Grayscale images of hand-written digits from 0 to 9).\n",
        "_[This first section is based on a notebook orignially contributed by: [afagarap](https://github.com/afagarap)]_\n",
        "\n",
        "\"Autoencoding\" is a data compression algorithm where the compression and decompression functions are 1) data-specific, 2) lossy, and 3) learned automatically from examples rather than engineered by a human. Additionally, in almost all contexts where the term \"autoencoder\" is used, the compression and decompression functions are implemented with neural networks.\n",
        "\n",
        " 1) Autoencoders are _data-specific_, which means that they will only be able to compress data similar to what they have been trained on. This is different from, say, the MPEG-2 Audio Layer III (MP3) compression algorithm, which only holds assumptions about \"sound\" in general, but not about specific types of sounds. An autoencoder trained on pictures of faces would do a rather poor job of compressing pictures of trees, because the features it would learn would be face-specific.\n",
        "\n",
        "2) Autoencoders are _lossy_, which means that the decompressed outputs will be degraded compared to the original inputs (similar to MP3 or JPEG compression). This differs from lossless arithmetic compression.\n",
        "\n",
        "3) Autoencoders are _learned automatically from data examples_, which is a useful property: it means that it is easy to train specialized instances of the algorithm that will perform well on a specific type of input. It doesn't require any new engineering, just appropriate training data.\n",
        "\n",
        "To build an autoencoder, you need three things: an encoding function, a decoding function, and a distance function between the amount of information loss between the compressed representation of your data and the decompressed representation (i.e. a \"loss\" function). The encoder and decoder will be chosen to be parametric functions (typically neural networks), and to be differentiable with respect to the distance function, so the parameters of the encoding/decoding functions can be optimize to minimize the reconstruction loss, using Stochastic Gradient Descent.\n",
        "\n",
        "In general, a neural network is a computational model that is used for finding a function describing the relationship between data features $x$ and its values or labels $y$, i.e. $y = f(x)$.\n",
        "An autoencoder is specific type of neural network, which consists of encoder and decoder components: (1) the **encoder**, which learns a compressed data representation $z$, and (2) the **decoder**, which reconstructs the data $\\hat{x}$ based on its idea $z$ of how it is structured:\n",
        "$$ z = f\\big(h_{e}(x)\\big)$$\n",
        "$$ \\hat{x} = f\\big(h_{d}(z)\\big),$$\n",
        "where $z$ is the learned data representation by encoder $h_{e}$, and $\\hat{x}$ is the reconstructed data by decoder $h_{d}$ based on $z$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zK2SZnszhSYk"
      },
      "source": [
        "## Setup\n",
        "We start by importing the libraries and functions that we will need."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S1sepk9uMddm"
      },
      "outputs": [],
      "source": [
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "\n",
        "#try:\n",
        "  # The %tensorflow_version magic only works in colab.\n",
        "  # tensorflow_version 2.x\n",
        "#except Exception:\n",
        "#  pass\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.keras.datasets import mnist\n",
        "\n",
        "print('TensorFlow version:', tf.__version__)\n",
        "print('Is Executing Eagerly?', tf.executing_eagerly())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CH3QqWQ1RiP2"
      },
      "source": [
        "## Autoencoder model\n",
        "\n",
        "The encoder and decoder are defined as:\n",
        "$$ z = f\\big(h_{e}(x)\\big)$$\n",
        "$$ \\hat{x} = f\\big(h_{d}(z)\\big),$$\n",
        "where $z$ is the compressed data representation generated by encoder $h_{e}$, and $\\hat{x}$ is the reconstructed data generated by decoder $h_{d}$ based on $z$.\n",
        "\n",
        "<div align=\"center\"><img src=\"https://github.com/benjaminirving/mlseminars-autoencoders/blob/master/imgs/d1.png?raw=1\" width=\"80%\"></div>\n",
        "\n",
        "In this figure, we take as input an image, and compress that image before decompressing it using a Dense network. We further define a simple model for this below."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fiYJ9BLW-w40"
      },
      "source": [
        "### Define an encoder layer\n",
        "\n",
        "The first component, the **encoder**, is similar to a conventional feed-forward network. However, it's function is not  predicting values (a _regression_ task) or categories (a _classification_ task). Instead, it's function is to learn a compressed data structure  $z$. We can implement the encoder layer as dense layers, as follows:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fdl5_0Z4-w41"
      },
      "outputs": [],
      "source": [
        "class Encoder(tf.keras.layers.Layer):\n",
        "    def __init__(self, intermediate_dim):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.hidden_layer = tf.keras.layers.Dense(units=intermediate_dim, activation=tf.nn.relu)\n",
        "        self.output_layer = tf.keras.layers.Dense(units=intermediate_dim, activation=tf.nn.relu)\n",
        "\n",
        "    def call(self, input_features):\n",
        "        activation = self.hidden_layer(input_features)\n",
        "        return self.output_layer(activation)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7WgR-V0--w44"
      },
      "source": [
        "The _encoding_ is done by passing data input $x$ to the encoder's hidden layer $h$ in order to learn the data representation $z = f(h(x))$.\n",
        "\n",
        "We first create an `Encoder` class that inherits the [`tf.keras.layers.Layer`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Layer) class to define it as a layer. The compressed layer $z$ is a _component_ of the autoencoder model.\n",
        "\n",
        "Analyzing the code, the `Encoder` layer is defined to have a single hidden layer of neurons (`self.hidden_layer`) to learn the input features. Then, we connect the hidden layer to a layer (`self.output_layer`) that encodes the learned activations to the lower dimensional layer for $z$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W9me1ZLq-w45"
      },
      "source": [
        "### Define a decoder layer\n",
        "\n",
        "The second component, the **decoder**, is also similar to a feed-forward network. However, instead of reducing data to lower dimension, it attempts to reverse the process, i.e. reconstruct the data $\\hat{x}$ from its lower dimension representation $z$ to its original dimension.\n",
        "\n",
        "The _decoding_ is done by passing the lower dimension representation $z$ to the decoder's hidden layer $h$ in order to reconstruct the data to its original dimension $\\hat{x} = f(h(z))$. We can implement the decoder layer as follows,"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m3za66lwMjWX"
      },
      "outputs": [],
      "source": [
        "class Decoder(tf.keras.layers.Layer):\n",
        "    def __init__(self, intermediate_dim, original_dim):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.hidden_layer = tf.keras.layers.Dense(units=intermediate_dim, activation=tf.nn.relu)\n",
        "        self.output_layer = tf.keras.layers.Dense(units=original_dim, activation=tf.nn.relu)\n",
        "\n",
        "    def call(self, code):\n",
        "        activation = self.hidden_layer(code)\n",
        "        return self.output_layer(activation)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SU-QtEUo-w4-"
      },
      "source": [
        "We now create a `Decoder` class that also inherits the `tf.keras.layers.Layer`.\n",
        "\n",
        "The `Decoder` layer is also defined to have a single hidden layer of neurons to reconstruct the input features $\\hat{x}$ from the learned representation $z$ by the encoder $f\\big(h_{e}(x)\\big)$. Then, we connect its hidden layer to a layer that decodes the data representation from lower dimension $z$ to its original dimension $\\hat{x}$. Hence, the \"output\" of the `Decoder` layer is the reconstructed data $\\hat{x}$ from the data representation $z$.\n",
        "\n",
        "Ultimately, the output of the decoder is the autoencoder's output.\n",
        "\n",
        "Now that we have defined the components of our autoencoder, we can finally build our model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uZRIHpkl-w5A"
      },
      "source": [
        "### Build the autoencoder model\n",
        "\n",
        "We can now build the autoencoder model by instantiating `Encoder` and `Decoder` layers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rl5HUez7-w5C"
      },
      "outputs": [],
      "source": [
        "class Autoencoder(tf.keras.Model):\n",
        "  def __init__(self, intermediate_dim, original_dim):\n",
        "    super(Autoencoder, self).__init__()\n",
        "    self.loss = []\n",
        "    self.encoder = Encoder(intermediate_dim=intermediate_dim)\n",
        "    self.decoder = Decoder(intermediate_dim=intermediate_dim, original_dim=original_dim)\n",
        "\n",
        "  def call(self, input_features):\n",
        "    code = self.encoder(input_features)\n",
        "    reconstructed = self.decoder(code)\n",
        "    return reconstructed"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "25zYoLju-w5G"
      },
      "source": [
        "As discussed above, the encoder's output is the input to the decoder, as it is written above (`reconstructed = self.decoder(code)`)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X9NBnWhSnvn4"
      },
      "source": [
        "## Reconstruction error\n",
        "\n",
        "To learn the compressed layer $z$, we define a loss function over the difference between the input data $x$ and the reconstruction of $x$, which is $\\hat{x}$.\n",
        "We call this comparison the reconstruction error function, a given by the following equation:\n",
        "$$ L = \\dfrac{1}{n} \\sum_{i=0}^{n-1} \\big(\\hat{x}_{i} - x_{i}\\big)^{2}$$\n",
        "where $\\hat{x}$ is the reconstructed data while $x$ is the original data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wmWZG-1qLP8p"
      },
      "outputs": [],
      "source": [
        "def loss(preds, real):\n",
        "  return tf.reduce_mean(tf.square(tf.subtract(preds, real)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KVjT6C2SqmH0"
      },
      "source": [
        "## Forward pass and optimization\n",
        "\n",
        "We will write a function for computing the forward pass, and applying a chosen optimization function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KNSJqjY7qnCe"
      },
      "outputs": [],
      "source": [
        "def train(loss, model, opt, original):\n",
        "  with tf.GradientTape() as tape:\n",
        "    preds = model(original)\n",
        "    reconstruction_error = loss(preds, original)\n",
        "  gradients = tape.gradient(reconstruction_error, model.trainable_variables)\n",
        "  gradient_variables = zip(gradients, model.trainable_variables)\n",
        "  opt.apply_gradients(gradient_variables)\n",
        "\n",
        "  return reconstruction_error"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8c4bNlGLrd5T"
      },
      "source": [
        "## The training loop\n",
        "\n",
        "Finally, we will write a function to run the training loop. This function will take arguments for the model, the optimization function, the loss, the dataset, and the training epochs.\n",
        "\n",
        "The training loop itself uses a `GradientTape` context defined in `train` for each batch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K8Sh1UaQrc5D"
      },
      "outputs": [],
      "source": [
        "def train_loop(model, opt, loss, dataset, epochs):\n",
        "  for epoch in range(epochs):\n",
        "    epoch_loss = 0\n",
        "    for step, batch_features in enumerate(dataset):\n",
        "      loss_values = train(loss, model, opt, batch_features)\n",
        "      epoch_loss += loss_values\n",
        "    model.loss.append(epoch_loss)\n",
        "    print('Epoch {}/{}. Loss: {}'.format(epoch + 1, epochs, epoch_loss.numpy()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8eCxbz9ZSwjr"
      },
      "source": [
        "## Process the dataset\n",
        "\n",
        "Now that we have defined our `Autoencoder` class, the loss function, and the training loop, let's import the dataset. We will normalize the pixel values for each example through dividing by maximum pixel value. We shall flatten the examples from 28 by 28 arrays to 784-dimensional vectors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eAN1ONp6MvI7"
      },
      "outputs": [],
      "source": [
        "(x_train, _), (x_test, _) = mnist.load_data()\n",
        "\n",
        "x_train = x_train / 255.\n",
        "\n",
        "x_train = x_train.astype(np.float32)\n",
        "x_train = np.reshape(x_train, (x_train.shape[0], 784))\n",
        "x_test = np.reshape(x_test, (x_test.shape[0], 784))\n",
        "\n",
        "training_dataset = tf.data.Dataset.from_tensor_slices(x_train).batch(256)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "268qdJGGTULP"
      },
      "source": [
        "## Train the model\n",
        "\n",
        "Now all we have to do is instantiate the autoencoder model and choose an optimization function, then pass the intermediate dimension and the original dimension of the images."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t8nw7mdKMxvb"
      },
      "outputs": [],
      "source": [
        "model = Autoencoder(intermediate_dim=128, original_dim=784)\n",
        "opt = tf.keras.optimizers.Adam(learning_rate=1e-2)\n",
        "\n",
        "train_loop(model, opt, loss, training_dataset, 20)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VioflTOhTnwl"
      },
      "source": [
        "## Plot the in-training performance\n",
        "\n",
        "Let's take a look at how the model performed during training in a couple of plots."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "azgmhikhM0EE"
      },
      "outputs": [],
      "source": [
        "plt.plot(range(20), model.loss)\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JccKWvNYTtUW"
      },
      "source": [
        "## Predictions\n",
        "\n",
        "Finally, we will look at some of the reconstructions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-JwpigAlM2dF"
      },
      "outputs": [],
      "source": [
        "number = 10  # how many digits we will display\n",
        "plt.figure(figsize=(20, 4))\n",
        "for index in range(number):\n",
        "    # display original\n",
        "    ax = plt.subplot(2, number, index + 1)\n",
        "    plt.imshow(x_test[index].reshape(28, 28))\n",
        "    plt.gray()\n",
        "    ax.get_xaxis().set_visible(False)\n",
        "    ax.get_yaxis().set_visible(False)\n",
        "\n",
        "    # display reconstruction\n",
        "    ax = plt.subplot(2, number, index + 1 + number)\n",
        "    plt.imshow(model(x_test)[index].numpy().reshape(28, 28))\n",
        "    plt.gray()\n",
        "    ax.get_xaxis().set_visible(False)\n",
        "    ax.get_yaxis().set_visible(False)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o4rkBaWN-w5r"
      },
      "source": [
        "## 1. Tasks for Basic Autoencoder Assignment\n",
        "\n",
        "As you may see after training this model, the reconstructed images are quite blurry. A number of things could be done to move forward from this point, e.g. adding more layers, or using a convolutional neural network architecture as the basis of the autoencoder, or use a different kind of autoencoder.\n",
        "\n",
        "- generate results for a more complex Dense architecturer\n",
        "- define more complex CNN  architectures (similar to as described) and compare their performance to that of the Dense models\n",
        "\n",
        "We are also going to move away from using Keras Sequential() models and introduce the [functional API](https://www.tensorflow.org/guide/keras/functional_api). This becomes important later (oustide the scope of this assignment) for implementing techniques such as residual connections.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def rmse_loss(preds, real):\n",
        "  return tf.sqrt(tf.reduce_mean(tf.square(tf.subtract(preds, real))))"
      ],
      "metadata": {
        "id": "yu2w6aBR2ubf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_img = tf.keras.layers.Input(shape=(28, 28, 1))\n",
        "\n",
        "# flatten the input to a vector\n",
        "x = tf.keras.layers.Flatten()(input_img)\n",
        "\n",
        "x = tf.keras.layers.Dense(256,activation='relu')(x)\n",
        "x = tf.keras.layers.Dense(64,activation='relu')(x)\n",
        "x = tf.keras.layers.Dense(32,activation='relu')(x)\n",
        "\n",
        "# at this point the representation is (4, 4, 8) i.e. 128-dimensional\n",
        "x = tf.keras.layers.Dense(64,activation='relu')(x)\n",
        "x = tf.keras.layers.Dense(256,activation='relu')(x)\n",
        "\n",
        "x = tf.keras.layers.Dense(784,activation='relu')(x)\n",
        "# turn the final layer back into an image\n",
        "x = tf.keras.layers.Reshape((28,28,1))(x)\n",
        "\n",
        "autoencoder = tf.keras.models.Model(input_img, x)\n",
        "autoencoder.compile(optimizer='adadelta', loss=rmse_loss)\n",
        "autoencoder.summary()"
      ],
      "metadata": {
        "id": "GRC2uF53x8-v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "run_name = \"basic_dense_xxxxx\"\n",
        "A.trainTaskOne(autoencoder,run_name,32)"
      ],
      "metadata": {
        "id": "mQ7dl9Dtx-SM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "28bbYL20Lrss"
      },
      "source": [
        "Since our inputs are images, it makes sense to try to use convolutional neural networks (CNNs) as encoders and decoders. In practical settings, autoencoders applied to images are always convolutional autoencoders -- they simply perform much better.\n",
        "\n",
        "- implement a CNN model, where the encoder will consist of a stack of Conv2D and MaxPooling2D layers (max pooling being used for spatial down-sampling), while the decoder will consist in a stack of Conv2D and UpSampling2D layers. To improve the quality of the reconstructed image, we use more filters per layer. Example model details are:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "21ArKpJSoa16"
      },
      "outputs": [],
      "source": [
        "input_img = tf.keras.layers.Input(shape=(28, 28, 1)) # adapt this if using `channels_first` image data format\n",
        "\n",
        "x = tf.keras.layers.Conv2D(16, (3, 3), activation='relu', padding='same')(input_img)\n",
        "x = tf.keras.layers.MaxPooling2D((2, 2), padding='same')(x)\n",
        "x = tf.keras.layers.Conv2D(8, (3, 3), activation='relu', padding='same')(x)\n",
        "x = tf.keras.layers.MaxPooling2D((2, 2), padding='same')(x)\n",
        "x = tf.keras.layers.Conv2D(8, (3, 3), activation='relu', padding='same')(x)\n",
        "encoded = tf.keras.layers.MaxPooling2D((2, 2), padding='same')(x)\n",
        "\n",
        "# at this point the representation is (4, 4, 8) i.e. 128-dimensional\n",
        "\n",
        "x = tf.keras.layers.Conv2D(8, (3, 3), activation='relu', padding='same')(encoded)\n",
        "x = tf.keras.layers.UpSampling2D((2, 2))(x)\n",
        "x = tf.keras.layers.Conv2D(8, (3, 3), activation='relu', padding='same')(x)\n",
        "x = tf.keras.layers.UpSampling2D((2, 2))(x)\n",
        "x = tf.keras.layers.Conv2D(16, (3, 3), activation='relu')(x)\n",
        "x = tf.keras.layers.UpSampling2D((2, 2))(x)\n",
        "decoded = tf.keras.layers.Conv2D(1, (3, 3), activation='sigmoid', padding='same')(x)\n",
        "\n",
        "autoencoder = tf.keras.models.Model(input_img, decoded)\n",
        "autoencoder.compile(optimizer='adadelta', loss=rmse_loss)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "run_name = \"basic_cnn_6_layers\"\n",
        "A.trainTaskOne(autoencoder,run_name,32)"
      ],
      "metadata": {
        "id": "gwZYsxnqmifm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qsRzfT-2LjxN"
      },
      "source": [
        "\n",
        "- train the model for 50 epochs and compare the results to the model where you use a dense encoding rather than convolutions.\n",
        "\n",
        "**Scoring**: 10 marks total\n",
        "- models [5 marks]:\n",
        "  - Dense, multi-layer model\n",
        "  - CNN complex model\n",
        "- results and discussion [5 marks]: present the results in a clear fashion and explain why the results were as obtained. Good experimental design and statistical significance testing will be rewarded.\n",
        "\n",
        "Submit your best Dense and CNN based models, as well as well as the explanation of results."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "''' SUBMISSION '''\n",
        "bestDenseModel = 'basic_dense_xxxxx'\n",
        "bestCNNModel = 'basic_cnn_6_layers'\n",
        "explanation = '''\n",
        "    The CNN model  performed significantly better across runs, etc\n",
        "'''\n",
        "\n",
        "A.make_submission_one(bestDenseModel, bestCNNModel, explanation)"
      ],
      "metadata": {
        "id": "Ak9VMP5LjX99"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4rmry8GsoFfi"
      },
      "source": [
        "## 2. Denoising autoencoder\n",
        "\n",
        "For this real-world application, we will use an autoencoder to remove noise from an image. To do this, we\n",
        "\n",
        "- learn a more robust representation by forcing the autoencoder to learn an input from a corrupted version of itself\n",
        "\n",
        "<div align=\"center\"><img src=\"https://github.com/benjaminirving/mlseminars-autoencoders/blob/master/imgs/d3.png?raw=1\" width=\"60%\"></div>\n",
        "\n",
        "The first step: generate synthetic noisy digits as follows: apply a gaussian noise matrix and clip the images between 0 and 1.\n",
        "\n",
        "This code will be handled within the Assignment class - but is also illustrated below for your own learning\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WbtNDoNhot-J"
      },
      "outputs": [],
      "source": [
        "\n",
        "(x_train, _), (x_test, _) = mnist.load_data()\n",
        "\n",
        "x_train = x_train.astype('float32') / 255.\n",
        "x_test = x_test.astype('float32') / 255.\n",
        "x_train = np.reshape(x_train, (len(x_train), 28, 28, 1))\n",
        "x_test = np.reshape(x_test, (len(x_test), 28, 28, 1))\n",
        "\n",
        "# Introduce noise with a probability factor of 0.5\n",
        "noise_factor = 0.5\n",
        "x_train_noisy = x_train + noise_factor + np.random.normal(loc=0.0, scale=1.0, size=x_train.shape)\n",
        "x_test_noisy = x_test + noise_factor + np.random.normal(loc=0.0, scale=1.0, size=x_test.shape)\n",
        "\n",
        "x_train_noisy = np.clip(x_train_noisy, 0., 1.)\n",
        "x_test_noisy = np.clip(x_test_noisy, 0., 1.)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qm59eENMpCeC"
      },
      "source": [
        "Next, plot some figures to see what the digits look like with noise added."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3tnUZbmOpAiz"
      },
      "outputs": [],
      "source": [
        "# Plot figures to show what the noisy digits look like\n",
        "n = 10\n",
        "plt.figure(figsize=(20, 2))\n",
        "for i in range(n):\n",
        "    ax = plt.subplot(1, n, i + 1)\n",
        "    plt.imshow(x_test_noisy[i].reshape(28, 28))\n",
        "    plt.gray()\n",
        "    ax.get_xaxis().set_visible(False)\n",
        "    ax.get_yaxis().set_visible(False)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "luZuk1rVpakh"
      },
      "source": [
        "## Models\n",
        "- Train CNN and Dense model for 50 epochs and compare the results to the model where you use a dense encoding rather than convolutions. Identify the best configurations for each, and explain their differences in performance."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def rmse_loss(preds, real):\n",
        "  return tf.sqrt(tf.reduce_mean(tf.square(tf.subtract(preds, real))))"
      ],
      "metadata": {
        "id": "NbWPKf-R4MgB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_img = tf.keras.layers.Input(shape=(28, 28, 1))\n",
        "\n",
        "# flatten the input to a vector\n",
        "x = tf.keras.layers.Flatten()(input_img)\n",
        "\n",
        "x = tf.keras.layers.Dense(256,activation='relu')(x)\n",
        "x = tf.keras.layers.Dense(64,activation='relu')(x)\n",
        "x = tf.keras.layers.Dense(32,activation='relu')(x)\n",
        "\n",
        "# at this point the representation is (4, 4, 8) i.e. 128-dimensional\n",
        "x = tf.keras.layers.Dense(64,activation='relu')(x)\n",
        "x = tf.keras.layers.Dense(256,activation='relu')(x)\n",
        "\n",
        "x = tf.keras.layers.Dense(784,activation='relu')(x)\n",
        "# turn the final layer back into an image\n",
        "x = tf.keras.layers.Reshape((28,28,1))(x)\n",
        "\n",
        "autoencoder = tf.keras.models.Model(input_img, x)\n",
        "autoencoder.compile(optimizer='adadelta', loss=rmse_loss)\n",
        "autoencoder.summary()"
      ],
      "metadata": {
        "id": "jZJvT1DOzPRe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7DvL3Q_gpfzm"
      },
      "outputs": [],
      "source": [
        "# This will train for 50 epochs\n",
        "run_name = \"basic_dense_6_layers\"\n",
        "A.trainTaskTwo(autoencoder,run_name,128)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_img = tf.keras.layers.Input(shape=(28, 28, 1)) # adapt this if using `channels_first` image data format\n",
        "\n",
        "x = tf.keras.layers.Conv2D(16, (3, 3), activation='relu', padding='same')(input_img)\n",
        "x = tf.keras.layers.MaxPooling2D((2, 2), padding='same')(x)\n",
        "x = tf.keras.layers.Conv2D(8, (3, 3), activation='relu', padding='same')(x)\n",
        "x = tf.keras.layers.MaxPooling2D((2, 2), padding='same')(x)\n",
        "x = tf.keras.layers.Conv2D(8, (3, 3), activation='relu', padding='same')(x)\n",
        "encoded = tf.keras.layers.MaxPooling2D((2, 2), padding='same')(x)\n",
        "\n",
        "# at this point the representation is (4, 4, 8) i.e. 128-dimensional\n",
        "\n",
        "x = tf.keras.layers.Conv2D(8, (3, 3), activation='relu', padding='same')(encoded)\n",
        "x = tf.keras.layers.UpSampling2D((2, 2))(x)\n",
        "x = tf.keras.layers.Conv2D(8, (3, 3), activation='relu', padding='same')(x)\n",
        "x = tf.keras.layers.UpSampling2D((2, 2))(x)\n",
        "x = tf.keras.layers.Conv2D(16, (3, 3), activation='relu')(x)\n",
        "x = tf.keras.layers.UpSampling2D((2, 2))(x)\n",
        "decoded = tf.keras.layers.Conv2D(1, (3, 3), activation='sigmoid', padding='same')(x)\n",
        "\n",
        "autoencoder = tf.keras.models.Model(input_img, decoded)\n",
        "autoencoder.compile(optimizer='adadelta', loss=rmse_loss)"
      ],
      "metadata": {
        "id": "zzCXZHvDzSAW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This will train for 50 epochs\n",
        "run_name = \"basic_cnn_6_layers\"\n",
        "A.trainTaskTwo(autoencoder,run_name,128)"
      ],
      "metadata": {
        "id": "fpK6UM7qzUB2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RySbrS-zssGb"
      },
      "source": [
        "**Scoring**: 10 marks total\n",
        "- models [5 marks]:\n",
        "  - Dense, multi-layer model\n",
        "  - CNN complex model\n",
        "- results and discussion [5 marks]: present the results in a clear fashion and explain why the results were as obtained. Good experimental design and statistical significance testing will be rewarded."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "''' SUBMISSION '''\n",
        "bestDenseModel = 'basic_dense_6_layers'\n",
        "bestCNNModel = 'basic_cnn_6_layers'\n",
        "explanation = '''\n",
        "    The CNN model  performed significantly better across runs, etc\n",
        "    ... deep autoencoders performed worse/better than shallow, ....\n",
        "'''\n",
        "\n",
        "A.make_submission_two(bestDenseModel, bestCNNModel, explanation)"
      ],
      "metadata": {
        "id": "c5wOHuYrrfTP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fU7s_1p-RGWR"
      },
      "source": [
        "**3. Text Reconstruction Application**\n",
        "\n",
        "You will now use the approach just described to reconstruct corrupted text. We will use a small dataset with grey-scale images of size $420 \\times 540$.  The steps required are as follows:\n",
        "\n",
        "- Apply this autoencoder approach (as just described) to the text data provided as noted below.\n",
        "- You must create an algorithm to clean the images in the test set, and report the error as RMSE (root-mean-square error).\n",
        "\n",
        "**Scoring**: 40 marks total\n",
        "- models [25 marks]:\n",
        "  - Dense, multi-layer model [5 marks];\n",
        "  - CNN basic model [5 marks];\n",
        "  - CNN complex model [15 marks].\n",
        "- results and discussion [15 marks]: present the results in a clear fashion and explain why the results were as obtained. Good experimental design and statistical significance testing will be rewarded.\n",
        "\n",
        "\n",
        "You will get full marks if you achieve RMSE < 0.005. Deductions are as follows:\n",
        "- -1: 0.01 $\\leq$ RMSE $\\leq$ 0.005\n",
        "- -5: 0.05 $\\leq$ RMSE $\\leq$ 0.01\n",
        "- -10:  RMSE $>$ 0.05\n",
        "\n",
        "Deductions are made if **none** of your models attain the speecifid RMSE losses above\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KIvKDS0xd1aL"
      },
      "source": [
        "Next you must build the model. I provide the code framework, with the model details left up to you."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rjx_N3Rnd8fF"
      },
      "outputs": [],
      "source": [
        "input_img = tf.keras.layers.Input(shape=(420,540,1))\n",
        "#enoder\n",
        "# enter encoder model here\n",
        "\n",
        "#decoder\n",
        "# enter decoder model model\n",
        "\n",
        "\n",
        "# Compile\n",
        "autoencoder.compile(optimizer='adam', loss=rmse_loss)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_img = tf.keras.layers.Input(shape=(420,540,1)) # adapt this if using `channels_first` image data format\n",
        "\n",
        "x = tf.keras.layers.Conv2D(16, (3, 3), activation='relu', padding='same')(input_img)\n",
        "x = tf.keras.layers.MaxPooling2D((2, 2), padding='same')(x)\n",
        "x = tf.keras.layers.Conv2D(8, (3, 3), activation='relu', padding='same')(x)\n",
        "x = tf.keras.layers.MaxPooling2D((2, 2), padding='same')(x)\n",
        "x = tf.keras.layers.Conv2D(8, (3, 3), activation='relu', padding='same')(x)\n",
        "encoded = tf.keras.layers.MaxPooling2D((2, 2), padding='same')(x)\n",
        "\n",
        "# at this point the representation is (4, 4, 8) i.e. 128-dimensional\n",
        "\n",
        "x = tf.keras.layers.Conv2D(8, (3, 3), activation='relu', padding='same')(encoded)\n",
        "x = tf.keras.layers.UpSampling2D((2, 2))(x)\n",
        "x = tf.keras.layers.Conv2D(8, (3, 3), activation='relu', padding='same')(x)\n",
        "x = tf.keras.layers.UpSampling2D((2, 2))(x)\n",
        "x = tf.keras.layers.Conv2D(16, (3, 3), activation='relu')(x)\n",
        "x = tf.keras.layers.UpSampling2D((2, 2))(x)\n",
        "decoded = tf.keras.layers.Conv2D(1, (3, 3), activation='sigmoid', padding='same')(x)\n",
        "\n",
        "autoencoder = tf.keras.models.Model(input_img, decoded)\n",
        "autoencoder.compile(optimizer='adadelta', loss=rmse_loss)"
      ],
      "metadata": {
        "id": "tB3WgqMV5D2n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aVXVNyo5fBMu"
      },
      "source": [
        "Next we have code to compile and run the model. Please modify the code below to fit your purposes."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# This will train for 50 epochs\n",
        "run_name = \"basic_cnn_x_layers\"\n",
        "A.trainTaskThree(autoencoder,run_name,128)"
      ],
      "metadata": {
        "id": "2yJS1o5ct7-Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2LR7l0fjfVnv"
      },
      "source": [
        "Next we can look at some predictions from the trained model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3-IkMCkPfZJm"
      },
      "outputs": [],
      "source": [
        "# Compute the prediction\n",
        "noisy_sample, clean_sample = A.getTask3SampleImage(\"train\")\n",
        "predicted_label = np.squeeze(autoencoder.predict(noisy_sample.reshape(1,420,540,1)))\n",
        "\n",
        "f, ax = plt.subplots(1,2, figsize=(10,8))\n",
        "ax[0].imshow(np.squeeze(clean_sample.reshape(1,420,540,1)), cmap='gray')\n",
        "ax[1].imshow(np.squeeze(predicted_label.astype('int8')), cmap='gray')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute the prediction\n",
        "noisy_sample, clean_sample = A.getTask3SampleImage(\"valid\")\n",
        "predicted_label = np.squeeze(autoencoder.predict(noisy_sample.reshape(1,420,540,1)))\n",
        "\n",
        "f, ax = plt.subplots(1,2, figsize=(10,8))\n",
        "ax[0].imshow(np.squeeze(clean_sample.reshape(1,420,540,1)), cmap='gray')\n",
        "ax[1].imshow(np.squeeze(predicted_label.astype('int8')), cmap='gray')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "37GVVjyz1Kw0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UZNhwBltfjXz"
      },
      "source": [
        "To complete this part of the project, you need to examine methods to improve the quality of the predictions. Report on methods to get better performance on this task."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "''' SUBMISSION '''\n",
        "bestDenseModel = 'basic_dense_6_layers'\n",
        "bestSimpleCNNModel = 'basic_cnn_x_layers'\n",
        "bestComplexCNNModel = 'basic_cnn_x_layers'\n",
        "explanation = '''\n",
        "    The CNN model  performed significantly better across runs, etc\n",
        "    ... deep autoencoders performed worse/better than shallow, ....\n",
        "'''\n",
        "\n",
        "A.make_submission_three(bestDenseModel, bestSimpleCNNModel, bestComplexCNNModel, explanation)"
      ],
      "metadata": {
        "id": "oUG67YeKv_xa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Assignment Submission\n",
        "1. Save this notebook (Ctrl+S) and download it with File->Download->Download .ipynb\n",
        "2. Submit both this .ipynb and **task1.tar.gz, task2.tar.gz and task3.tar.gz** to Canvas - do not zip, tar or otherwsie combine or modify these files"
      ],
      "metadata": {
        "id": "Bf7fa5je8Vlx"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lXI9G4whP-sV"
      },
      "source": [
        "## References\n",
        "* Martn Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng Chen, Craig Citro, Greg S. Corrado, Andy Davis, Jeffrey Dean, Matthieu Devin, Sanjay Ghemawat, Ian Goodfellow, Andrew Harp, Geoffrey Irving, Michael Isard, Rafal Jozefowicz, Yangqing Jia, Lukasz Kaiser, Manjunath Kudlur, Josh Levenberg, Dan Man, Mike Schuster, Rajat Monga, Sherry Moore, Derek Murray, Chris Olah, Jonathon Shlens, Benoit Steiner, Ilya Sutskever, Kunal Talwar, Paul Tucker, Vincent Vanhoucke, Vijay Vasudevan, Fernanda Vigas, Oriol Vinyals, Pete Warden, Martin Wattenberg, Martin Wicke, Yuan Yu, and Xiaoqiang Zheng. TensorFlow: Large-scale machine learning on heterogeneous systems, 2015. Software available from [tensorflow.org](https://tensorflow.org/).\n",
        "* Chollet, F. (2016, May 14). Building Autoencoders in Keras. Retrieved March 19, 2019, from https://blog.keras.io/building-autoencoders-in-keras.html\n",
        "* Goodfellow, I., Bengio, Y., & Courville, A. (2016). Deep learning. MIT press."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "collapsed_sections": [
        "FebDKAJxxxTd"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}