{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<table align=\"center\">\n",
        "  <td align=\"center\"><a target=\"_blank\" href=\"https://colab.research.google.com/github/andrew-nash/CS6421-labs/blob/main/Lab8.ipynb\">\n",
        "        <img src=\"https://i.ibb.co/2P3SLwK/colab.png\"  style=\"padding-bottom:5px;\" />Run in Google Colab</a></td>\n",
        "  <td align=\"center\"><a target=\"_blank\" href=\"https://github.com/andrew-nash/CS6421-labs/blob/main/Lab8.ipynb\">\n",
        "        <img src=\"https://i.ibb.co/xfJbPmL/github.png\"  height=\"70px\" style=\"padding-bottom:5px;\"  />View Source on GitHub</a></td>\n",
        "</table>\n",
        "\n",
        "# Lab 8 - RNNs\n",
        "\n",
        "Based on https://www.tensorflow.org/guide/keras/working_with_rnns, https://www.tensorflow.org/text/tutorials/text_classification_rnn\n",
        "\n",
        "## Introduction\n",
        "\n",
        "Recurrent neural networks (RNN) are a class of neural networks that is powerful for\n",
        "modeling sequence data such as time series or natural language.\n",
        "\n",
        "Schematically, a RNN layer uses a `for` loop to iterate over the timesteps of a\n",
        "sequence, while maintaining an internal state that encodes information about the\n",
        "timesteps it has seen so far.\n",
        "\n",
        "The Keras RNN API is designed with a focus on:\n",
        "\n",
        "- **Ease of use**: the built-in `keras.layers.RNN`, `keras.layers.LSTM`,\n",
        "`keras.layers.GRU` layers enable you to quickly build recurrent models without\n",
        "having to make difficult configuration choices.\n",
        "\n",
        "- **Ease of customization**: You can also define your own RNN cell layer (the inner\n",
        "part of the `for` loop) with custom behavior, and use it with the generic\n",
        "`keras.layers.RNN` layer (the `for` loop itself). This allows you to quickly\n",
        "prototype different research ideas in a flexible way with minimal code."
      ],
      "metadata": {
        "id": "KXTUKC0M8nkk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "R-zvpo0X8hOs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wandb -qU\n",
        "%load_ext tensorboard\n",
        "%tensorboard --logdir tboard"
      ],
      "metadata": {
        "id": "DPps_oFV-zWi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import wandb\n",
        "from wandb.keras import WandbMetricsLogger, WandbModelCheckpoint\n",
        "wandb.login()"
      ],
      "metadata": {
        "id": "WPCIc7pj-12S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset\n",
        "\n",
        "The example we will see will involve using RNNs for **text classification**.\n",
        "\n",
        "This text classification tutorial trains a recurrent neural network on the IMDB large movie review dataset for sentiment analysis. (Details at https://ai.stanford.edu/~amaas/data/sentiment/). The task here is to, given a review of a particular movie, identify whether that review is either positive or negative."
      ],
      "metadata": {
        "id": "1l2MCIUK89HD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yQwpRJUY8Ypf"
      },
      "outputs": [],
      "source": [
        "dataset, info = tfds.load('imdb_reviews', with_info=True,\n",
        "                          as_supervised=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Looking at a sample from this data:"
      ],
      "metadata": {
        "id": "_i9mD9BS_KRE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset, valid_dataset = dataset['train'], dataset['test']\n",
        "\n",
        "train_dataset.element_spec"
      ],
      "metadata": {
        "id": "pxp0LNmd_xVV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for example, label in train_dataset.take(1):\n",
        "  print('text: ', example.numpy())\n",
        "  print('label: ', label.numpy())"
      ],
      "metadata": {
        "id": "ZDo3sMQB-7up"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally , shuffle and batch the data"
      ],
      "metadata": {
        "id": "SXWMOSUk_N16"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "BUFFER_SIZE = 10000\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "train_dataset = train_dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
        "valid_dataset = valid_dataset.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "for example, label in train_dataset.take(1):\n",
        "  print('texts: ', example.numpy()[:3])\n",
        "  print()\n",
        "  print('labels: ', label.numpy()[:3])"
      ],
      "metadata": {
        "id": "nQRQ79lk_FUJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For this to be usable input to a deep model, this textual data must be converted to some form of numeric input - the details of the best way to accomplish this are outside the scope of this lab. More can be found about this at:\n",
        "\n",
        "1. https://medium.com/data-science-in-your-pocket/text-vectorization-algorithms-in-nlp-109d728b2b63\n",
        "2. https://www.tensorflow.org/api_docs/python/tf/keras/layers/TextVectorization\n",
        "\n",
        "#### Text Vectorization\n",
        "\n",
        "Keras includes a pre-defined model for vectorizing texts.\n",
        "\n",
        "This model applies the folloing processing:\n",
        "\n",
        "1. Standardize each example (usually lowercasing + punctuation stripping)\n",
        "2. Split each example into substrings (usually words)\n",
        "3. Recombine substrings into tokens (usually ngrams) (by default, this is skipped)\n",
        "4. Index tokens (associate a unique int value with each token)\n",
        "5. Transform each example using this index, either into a vector of ints or a dense float vector.\n",
        "\n",
        "The vocabulary size corresponds to the maximum number of tokens that are allowed to be included in the vectorization - i.e., the maximum size of the encoded vector.\n",
        "\n",
        "The following code trains a TextVectorization with a given maximum number of tokens on the IMDB data:"
      ],
      "metadata": {
        "id": "m4Ven4bLAkmb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "VOCAB_SIZE = 1000\n",
        "encoder = tf.keras.layers.TextVectorization(\n",
        "    max_tokens=VOCAB_SIZE)\n",
        "encoder.adapt(train_dataset.map(lambda text, label: text))"
      ],
      "metadata": {
        "id": "b6ONalH6_R6z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can look at the some of the tokens learned ([UNK] corresponds to all words outside the vocabulary)"
      ],
      "metadata": {
        "id": "kts9JvwUDCXx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = np.array(encoder.get_vocabulary())\n",
        "vocab[:20]"
      ],
      "metadata": {
        "id": "uhgoEJPFBtlm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can also now look at some examples of vectoized sentences"
      ],
      "metadata": {
        "id": "eVcQJOsBDNby"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "encoded_example = encoder(example)[:3].numpy()\n",
        "encoded_example.shape,encoded_example"
      ],
      "metadata": {
        "id": "WAZBms42BwTN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for n in range(3):\n",
        "  print(\"Original: \", example[n].numpy())\n",
        "  print(\"Round-trip: \", \" \".join(vocab[encoded_example[n]]))\n",
        "  print(\"First 10 elements of vectorized representation:\", encoded_example[n][:10])\n",
        "  print()"
      ],
      "metadata": {
        "id": "VEkruoohB4gL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We are now ready to train a model!\n",
        "\n",
        "## Embedding layers (https://www.tensorflow.org/text/guide/word_embeddings)\n",
        "\n",
        "The Vectorized inputs are a much more workable format for modelling - but can still be improved. Each element of the vector corresponds to the index of a particular word in the vocabulary (you can think of it as that word's unique integer id).\n",
        "\n",
        "This does not in itself capture any sense of the *meaning* of the involved words.\n",
        "\n",
        "An embedding layer is used to create a vector to represent each word\n",
        "\n",
        "Conside the following example:\n",
        "\n",
        "We have a vocabulary:\n",
        "\n",
        "`{cat,mat,on,sat,the}`\n",
        "\n",
        "And a sentence `\"the cat sat\"`.\n",
        "\n",
        "This sentence is vectorized as:\n",
        "\n",
        "`[4,0,3]`\n",
        "\n",
        "An embedding would learn a unique **vector** for each word, such as:\n",
        "\n",
        "\n",
        "<img src='https://www.tensorflow.org/static/text/guide/images/one-hot.png' width=30%/>\n",
        "\n",
        "\n",
        "<img src='https://www.tensorflow.org/static/text/guide/images/embedding2.png' width=40%/>\n",
        "\n",
        "The embedding layer can be thought of as a smaller deep model that learns vectors that learn features (i.e. meaning) of the words in a vocabulary.\n",
        "\n",
        "If we take the 4-D embedding, this sentence would become something like:\n",
        "\n",
        "\n",
        "[ [-1.054,-0.75, 0.065,2.5] (the), [1.2,-0.1,4.3,3.2] (cat), [-0.75,0.5,1.0,5.0] (sat) ]\n",
        "\n",
        "\n",
        "### Modelling\n",
        "\n",
        "We now have a sequence of 'Embdedded Vectors'. Observe that this is temporal data - the embedded vectors are the words of the original sentence, in order.\n",
        "\n",
        "Therefore, we need a temporal model - such as the RNN (the bidirectional block of the diagram below)\n",
        "\n",
        "<img src='https://www.tensorflow.org/static/text/tutorials/images/bidirectional.png' />"
      ],
      "metadata": {
        "id": "F9g5pKOOEIxb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = tf.keras.Sequential()\n",
        "# for simplicity, we will bake the text vectorization and embedding into our model\n",
        "model.add(encoder)\n",
        "# Add the text embedding to learn feature vectors on words\n",
        "# the output_dim is the size of the learned vector (in the above example, it would be 4)\n",
        "# mask_zero=True allows the sizes of input sentences to be variable\n",
        "model.add(tf.keras.layers.Embedding(input_dim=len(encoder.get_vocabulary()), output_dim=64,mask_zero=True))\n",
        "model.add(tf.keras.layers.SimpleRNN(32))\n",
        "model.add(tf.keras.layers.Dense(64, activation='relu'))\n",
        "model.add(tf.keras.layers.Dense(1, activation='relu'))"
      ],
      "metadata": {
        "id": "LUj0z2O-DTH4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "id": "NHeFsy1EKXey"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Even before training, we can see the model in action:"
      ],
      "metadata": {
        "id": "e0eB_NhPK1A4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sample_text = ('The movie was cool. The animation and the graphics '\n",
        "               'were out of this world. I would recommend this movie.')\n",
        "predictions = model.predict(np.array([sample_text]))\n",
        "print(predictions[0])"
      ],
      "metadata": {
        "id": "XMjixUMuKtdm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "run_name=\"Basic RNN\"\n",
        "#   TextVectorizer is incompatible with tboard weight histograms, so we have to disable these\n",
        "#   in practice, for this reason it can be better to keep TextVectorizer outside the trainable model\n",
        "tensorboard_callback = tf.keras.callbacks.TensorBoard(f\"./tboard/{run_name}\", histogram_freq=0)\n",
        "wandb.init(\n",
        "        project = \"Lab8\",\n",
        "        name =   run_name)\n",
        "\n",
        "model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
        "              optimizer=tf.keras.optimizers.Adam(1e-4),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "history = model.fit(train_dataset, epochs=1,\n",
        "                    validation_data=valid_dataset,\n",
        "                    validation_steps=30, callbacks=[tensorboard_callback,WandbMetricsLogger()])"
      ],
      "metadata": {
        "id": "lb4KecvdKxJH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_text = ('The movie was cool. The animation and the graphics '\n",
        "               'were out of this world. I would recommend this movie.')\n",
        "predictions = model.predict(np.array([sample_text]))\n",
        "print(predictions)"
      ],
      "metadata": {
        "id": "TcijElzTLjGE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_text = ('The movie was not good. The animation and the graphics '\n",
        "               'were terrible. I would not recommend this movie.')\n",
        "predictions = model.predict(np.array([sample_text]))\n",
        "print(predictions)"
      ],
      "metadata": {
        "id": "337WEoigLkLK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## RNN Hyper-parameters\n",
        "\n",
        "RNN's can be configured in many ways:\n",
        "\n",
        "When we say a RNN is set up as XX-to-YY, this means that it takes inputs over XX time steps, and makes outputs over YY time steps. I.e., one-to-many means that the RNN will take inputs in a single time step (in this case, the output of the RNN will be recycled to its input at each time step), and output a series of values.\n",
        "\n",
        "<img src=\"https://api.wandb.ai/files/ayush-thakur/images/projects/103390/4fc355be.png\"/>\n",
        "\n",
        "<img src='https://miro.medium.com/v2/resize:fit:640/format:webp/0*VO4DW_vN7ldqgEZg.png'/>\n",
        "\n",
        "When we have a single, simple RNN layer such as\n",
        "\n",
        "`model.add(tf.keras.layers.SimpleRNN(32))`\n",
        "\n",
        "This can be considered, with muliple time steps as input, as a many-to-one model - it will output a single 32 element vector at the last timestep. In our model above, this acted as input to a series of Dense layers to perform classification.\n",
        "\n",
        "If we want to turb this into a many-to-many model, we can use:\n",
        "\n",
        "`model.add(tf.keras.layers.SimpleRNN(32, return_sequences=True))`\n",
        "\n",
        "This will instruct the RNN to make an output at each time step. A major advantage of this is that it allows us to *stack* RNNs, with the output of one being passed to another at each time step.\n",
        "\n",
        "An example of 3 Stacked LSTM models for a many-to-one problem:\n",
        "\n",
        "<img src='https://miro.medium.com/v2/resize:fit:720/format:webp/0*sBfgsRRLyknLfca7.jpg'/>\n",
        "\n",
        "If we were to `return_sequences=True` in our final recurrent layer, the model could become many-to-many!\n",
        "\n",
        "Other hyper-parameters of the RNN/LSTM/GRU are also alterable:\n",
        "\n",
        "https://www.tensorflow.org/api_docs/python/tf/keras/layers/LSTM\n",
        "\n",
        "1. activation\n",
        "2. Recurrent activation\n",
        "2. use_bias\n",
        "3. regularizers\n",
        "\n",
        "etc."
      ],
      "metadata": {
        "id": "BFMNkxHXLlOC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = tf.keras.Sequential()\n",
        "# for simplicity, we will bake the text vectorization and embedding into our model\n",
        "model.add(encoder)\n",
        "# Add the text embedding to learn feature vectors on words\n",
        "# the output_dim is the size of the learned vector (in the above example, it would be 4)\n",
        "# mask_zero=True allows the sizes of input sentences to be variable\n",
        "model.add(tf.keras.layers.Embedding(input_dim=len(encoder.get_vocabulary()), output_dim=64,mask_zero=True))\n",
        "model.add(tf.keras.layers.LSTM(32, return_sequences=True))\n",
        "model.add(tf.keras.layers.LSTM(16, return_sequences=True))\n",
        "model.add(tf.keras.layers.LSTM(8))\n",
        "model.add(tf.keras.layers.Dense(64, activation='relu'))\n",
        "model.add(tf.keras.layers.Dense(1))"
      ],
      "metadata": {
        "id": "K34Nzj7BLki6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "run_name=\"Stacked LSTM\"\n",
        "#   TextVectorizer is incompatible with tboard weight histograms, so we have to disable these\n",
        "#   in practice, for this reason it can be better to keep TextVectorizer outside the trainable model\n",
        "tensorboard_callback = tf.keras.callbacks.TensorBoard(f\"./tboard/{run_name}\", histogram_freq=0)\n",
        "wandb.init(\n",
        "        project = \"Lab8\",\n",
        "        name =   run_name)\n",
        "\n",
        "model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
        "              optimizer=tf.keras.optimizers.Adam(1e-4),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "history = model.fit(train_dataset, epochs=20,\n",
        "                    validation_data=valid_dataset,\n",
        "                    validation_steps=30, callbacks=[tensorboard_callback,WandbMetricsLogger()])"
      ],
      "metadata": {
        "id": "_EelqCFISNWp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_text = ('The movie was cool. The animation and the graphics '\n",
        "               'were out of this world. I would recommend this movie.')\n",
        "predictions = model.predict(np.array([sample_text]))\n",
        "print(predictions)"
      ],
      "metadata": {
        "id": "zzVBvDGrUMHU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_text = ('Great stuff, loved it. Best thing ever.')\n",
        "predictions = model.predict(np.array([sample_text]))\n",
        "print(predictions)"
      ],
      "metadata": {
        "id": "WZjQ_IqWUMha"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_text = (\"That was the worst thing I've seen in ages\")\n",
        "predictions = model.predict(np.array([sample_text]))\n",
        "print(predictions)"
      ],
      "metadata": {
        "id": "bPZr11m4cgeq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_text = (\"Three hours of my life that I won't get back\")\n",
        "predictions = model.predict(np.array([sample_text]))\n",
        "print(predictions)"
      ],
      "metadata": {
        "id": "AH8McnrHdX15"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}