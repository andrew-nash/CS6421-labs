{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<table align=\"center\">\n",
        "  <td align=\"center\"><a target=\"_blank\" href=\"https://colab.research.google.com/github/andrew-nash/CS6421-labs/blob/main/Lab2.ipynb\">\n",
        "        <img src=\"https://i.ibb.co/2P3SLwK/colab.png\"  style=\"padding-bottom:5px;\" />Run in Google Colab</a></td>\n",
        "  <td align=\"center\"><a target=\"_blank\" href=\"https://github.com/andrew-nash/CS6421-labs/blob/main/Lab2.ipynb\">\n",
        "        <img src=\"https://i.ibb.co/xfJbPmL/github.png\"  height=\"70px\" style=\"padding-bottom:5px;\"  />View Source on GitHub</a></td>\n",
        "</table>\n",
        "\n",
        "\n",
        "# Basic Model Optimiztion with TensorFlow\n",
        "\n",
        "During the first lab, we saw examples of defining simple feed-forward models containing a single layer of neurons.\n",
        "\n",
        "In this lab, we will look at\n",
        "\n",
        "1. The process of defining loss function, calculating gradients and backpropogating weight updates\n",
        "2. How to monitor the training process - assessing loss and accuracy over time, and comparing the overall performane of models with different hyper-parameters\n",
        "\n",
        "We will not focus on specific data pro-processing, or the optimality/sub-optimality of any particular modelling choices - these are topics for future labs, that will use the techniques discussed here"
      ],
      "metadata": {
        "id": "ZKhQHmVVOMgt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Recap on a simple TensorFlow model from last week"
      ],
      "metadata": {
        "id": "qCnzw0A9rRL3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf"
      ],
      "metadata": {
        "id": "Rq0UZ2tPQB45"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jm-SwctSOLg5"
      },
      "outputs": [],
      "source": [
        "### Defining a neural network using the Sequential API ###\n",
        "\n",
        "# Import relevant packages\n",
        "from tensorflow.keras import Sequential\n",
        "\n",
        "# Define the number of outputs\n",
        "n_output_nodes = 3\n",
        "\n",
        "# First define the model\n",
        "model = Sequential()\n",
        "\n",
        "# Remember: dense layers are defined by the parameters W and b!\n",
        "dense_layer = tf.keras.layers.Dense(10)\n",
        "\n",
        "# Add the dense layer to the model\n",
        "model.add(dense_layer)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have seen that we can perform inference on this model with"
      ],
      "metadata": {
        "id": "2Ue--IrlPq1p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x_input = tf.constant([[1,2.]], shape=(1,2))\n",
        "\n",
        "print(model(x_input))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Gcbvp8PPqIm",
        "outputId": "02b06991-687a-4f47-b9db-9d985eceb29b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[[-1.3462192   0.26335108 -1.2146258  -0.40529662 -0.87654626 -0.10545951\n",
            "  -1.0769374   0.6932813   1.4673295   1.847245  ]], shape=(1, 10), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The net question is, how can we perform *training* on this model, as discussed in class?\n",
        "\n",
        "For the example of a supervised model:\n",
        "\n",
        "1. We need to define some input data, with labels associated to each input. This data should be split into train and test partitions.\n",
        "2. We need to define a *loss function*, to compute a measure of difference between predicted values and true labels\n",
        "3. For each prediction and loss value, we must then *backpropogate* weight updates baack through the network\n",
        "\n",
        "Luckily for us, TensorFlow can abstract much of this process into simple functions"
      ],
      "metadata": {
        "id": "-bY8qXhDQWDq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Input Data"
      ],
      "metadata": {
        "id": "YJKewA9nRzsn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For this example, we will use a pre-loaded dataset from TensorFlow - the MNIST (Modified NIST) dataset. This is a set of 70,000 28x28 greyscale images, with associated labels, of handwritten digits (0-9).\n",
        "\n",
        "In this case TensorFlow has already split up the dataset to give us 60k images for training, and a separate 10k for evaluation.\n",
        "\n",
        "Proper data loading, handling and pro-processing will be discussed more fully in the next lab - for now we will just consider this dataset as an example\n",
        "\n"
      ],
      "metadata": {
        "id": "noKneDjKSPno"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mnist = tf.keras.datasets.mnist\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "x_train, x_test = x_train / 255.0, x_test / 255.0"
      ],
      "metadata": {
        "id": "SHpdO7bmR2S8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define a simple model to train\n",
        "\n",
        "We need to specify the \"shape\" of the input that will be passsed to the model. In this case, we pass the image as a 'flat' vector (rather than as a matrix) to simplify the code.\n"
      ],
      "metadata": {
        "id": "HVLD9tOjXwns"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "dense_layer = tf.keras.layers.Dense(10, input_shape=[784])\n",
        "model.add(dense_layer)"
      ],
      "metadata": {
        "id": "6RZikHO3Xvci"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U5O5WXCXb8IK",
        "outputId": "fc45664a-d92d-409f-e938-c637fde7ca1a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_9\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_8 (Dense)             (None, 10)                7850      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 7850 (30.66 KB)\n",
            "Trainable params: 7850 (30.66 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loss Function"
      ],
      "metadata": {
        "id": "1Pk-ARnyR1_K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this simple example, the predicted values are a single integer - the predicted digit of the inputted image."
      ],
      "metadata": {
        "id": "-yM3QM5ZT3-4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def mean_absolute_error(true_label, predicted_label):\n",
        "    abs_err = tf.abs(tf.subtract(true_label, predicted_label))\n",
        "    return tf.reduce_mean(abs_err)"
      ],
      "metadata": {
        "id": "x5mubNtLR5D9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def mean_squared_error(y_true, y_pred):\n",
        "    ''' TODO Implement Mean Squared Error Loss\n",
        "    '''\n",
        "    return"
      ],
      "metadata": {
        "id": "yKspAd-qUu0c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Back-propogation"
      ],
      "metadata": {
        "id": "5hiNRyCPR5Ud"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code we saw last week can be used to automatically differentiate our loss function with respect to the weight values, and from there apply updates to our weights.\n",
        "\n",
        "### First, we must track the forward pass with tf.GradientTape()"
      ],
      "metadata": {
        "id": "BOt4kzz3R8Yc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sample_image = x_train[0].reshape(1,784)\n",
        "sample_label = y_train[0]\n",
        "\n",
        "with tf.GradientTape() as tape:\n",
        "    prediction = ''' TODO: Perform inference on the sample image '''\n",
        "    loss_value = ''' TODO: compute the loss under mse '''\n",
        "\n",
        "print(\"Pre-update loss:\", loss_value)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HpKW7JuWWNpm",
        "outputId": "d845afd0-a33a-4744-ab29-3149c4e571ed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pre-update loss: tf.Tensor(189.28352, shape=(), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Then, find the gradients"
      ],
      "metadata": {
        "id": "cX85VHuTXFdA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gradients = tape.gradient(loss_value, model.trainable_variables)"
      ],
      "metadata": {
        "id": "2kcKIwB9XD0f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### We can now use these gradients to apply weight updates\n",
        "\n",
        "$W_{t+1} = W_t-\\lambda \\star \\nabla_W $\n",
        "\n",
        "Where $\\lambda$ is the *learning-rate* hyperparameter"
      ],
      "metadata": {
        "id": "27Zj2w0ZYgQj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lr = tf.constant(0.001)\n",
        "# UPDATING THE WEIGHTS (of layer 1)\n",
        "model.trainable_variables[0].assign(model.trainable_variables[0] - tf.multiply(lr,gradients[0]))\n",
        "# UPDATING THE BIASES (of layer 1)\n",
        "model.trainable_variables[1].assign(model.trainable_variables[1] - tf.multiply(lr,gradients[1]))"
      ],
      "metadata": {
        "id": "gIRLU1goZfyF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Re-perform the inference, and see if the loss has reduced"
      ],
      "metadata": {
        "id": "HWWxZDASc9at"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "new_prediction = ''' TODO: Perform inference on the sample image '''\n",
        "new_loss_value = ''' TODO: compute the loss '''\n",
        "print(\"Post-update loss:\", new_loss_value)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EeQfZIlVc6Dn",
        "outputId": "adf3ddce-6e05-49cf-e2c4-3349013f8699"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Post-update loss: tf.Tensor(182.356, shape=(), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we can see, the model has completed successful forward and backward passes through the model"
      ],
      "metadata": {
        "id": "PbsCGtdUe7eL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Backpropogation, but simpler\n",
        "\n",
        "TensorFlow has a much simpler method to perform back-propogation, which is especially important for more complicated models"
      ],
      "metadata": {
        "id": "q8TGXofEbbvG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "dense_layer = tf.keras.layers.Dense(10, input_shape=[784])\n",
        "model.add(dense_layer)"
      ],
      "metadata": {
        "id": "vtlBKOGfbZRG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(\n",
        "    optimizer = \"SGD\",\n",
        "    loss = \"mean_squared_error\",\n",
        "    metrics = ['accuracy']\n",
        ")"
      ],
      "metadata": {
        "id": "kruIuaHxfFX0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(\n",
        "    x_train.reshape(-1, 784),\n",
        "    y_train,\n",
        "    epochs=10,\n",
        "    batch_size = 128,\n",
        "    validation_data = (x_test.reshape(-1, 784),y_test)\n",
        ")"
      ],
      "metadata": {
        "id": "GnZLaidhfSH8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This provides with a quick and easy way to initialise the training process, and gives us some nicely formatted loss & accuracy metrics.\n",
        "\n",
        "One of the most useful additions to these training 'wrapper' functions are Callbacks - custom functions that enable us to log all sorts of other useful information to monitor the model training process\n"
      ],
      "metadata": {
        "id": "HotVbAKdfxY_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Computation Graph \\& Monitoring Training with TensorBoard\n",
        "\n",
        "TensorBoard is TensorFlow's official (https://www.tensorflow.org/tensorboard/get_started) monitoring tool. This can be installed and used as a standalone package on your machine to run alongside any TensorFlow model training - or it can be used in the form of a Jupyter Notebook extension"
      ],
      "metadata": {
        "id": "PGXDXMVGOSws"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext tensorboard"
      ],
      "metadata": {
        "id": "c7xBjIBDOcd0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "dense_layer = tf.keras.layers.Dense(10, input_shape=[784])\n",
        "model.add(dense_layer)\n",
        "\n",
        "model.compile(\n",
        "    optimizer = \"SGD\",\n",
        "    loss = \"mean_squared_error\",\n",
        "    metrics = ['accuracy']\n",
        ")"
      ],
      "metadata": {
        "id": "F4wPc_jCg83-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "While your TensorBoard logs can be located and formatted however you want, it is highly recommended to place logs in a timestamped directory keep everything well organised"
      ],
      "metadata": {
        "id": "w9vFPMbehAXL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import datetime\n",
        "log_dir = \"logs/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")"
      ],
      "metadata": {
        "id": "tIef4evghQXG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tensorboard can now be start, specifying the location in which logs will be placed"
      ],
      "metadata": {
        "id": "hfNBgREPicCB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%tensorboard --logdir logs"
      ],
      "metadata": {
        "id": "riIlQ9CHia29"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The TensorBoard Callback is an Object that attaches to a particular model, and streams data live to TensorBoard application - so we get real-time visualisation of our metrics.\n",
        "\n",
        "The frequency at which this data is streamed is specified in a number of epochs with _freq arguments\n",
        "\n",
        "E.g.:\n",
        "\n",
        "update_freq = 1 means that the losses and metrics are sent to tensorboard at the end of every epoch\n",
        "\n",
        "histogram_freq = 1 means that the model weights are sent to tensorboard at the end of every epoch"
      ],
      "metadata": {
        "id": "GERgNo0ahXVL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, update_freq=1, histogram_freq=1)"
      ],
      "metadata": {
        "id": "9ti6uuhjhWPV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(\n",
        "    x_train.reshape(-1, 784),\n",
        "    y_train,\n",
        "    epochs=10,\n",
        "    batch_size = 128,\n",
        "    validation_data = (x_test.reshape(-1, 784),y_test),\n",
        "    callbacks = [tensorboard_callback]\n",
        ")"
      ],
      "metadata": {
        "id": "bzzRKYAqg4jg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Interpreting the TensorBoard Data\n",
        "\n",
        "In your own time, it is well worth exploring this tool to see the sorts of information that it can visualise. Some of the most important aspects:\n",
        "\n",
        "1. Plots of train and test loss (and other metrics such as accuracy) over time\n",
        "2. A complete representation of the computation graph of the model\n",
        "3. Distributions of the weight and bias values over time"
      ],
      "metadata": {
        "id": "TU0P4jGgi5i_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dashboards with *Weights And Biases* (wandb.ai)  \n",
        "\n",
        "Based on the tutorial from: https://colab.research.google.com/github/wandb/examples/blob/master/colabs/intro/Intro_to_Weights_&_Biases_keras.ipynb#scrollTo=ru06gHFl1B0M\n",
        "\n",
        "While TensorBoard is an excellent tool for monitoring and assessing the model parameters (weights & biased) in training, there are other monitoring tools which are useful for comparing the performance of models for different choices of hyper-paramters (such as number of layers, choice of activation function, epochs of training, optimization function, etc)\n",
        "\n",
        "An example of such a tool is `Weights And Biases` - wanb.ai . It focuses on Tableau-like dashboards, generated automatically from your models."
      ],
      "metadata": {
        "id": "2VIPmZSLOfHO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Installation"
      ],
      "metadata": {
        "id": "mopcRflxl1U7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wandb -qU"
      ],
      "metadata": {
        "id": "4IOycq2gOqm3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import wandb\n",
        "from wandb.keras import WandbMetricsLogger, WandbModelCheckpoint"
      ],
      "metadata": {
        "id": "xqpsg7tPm8Dc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, create an account at https://wandb.ai\n",
        "\n",
        "Once that's done, copy the API key for your account into the text box that appears when running the cell below"
      ],
      "metadata": {
        "id": "G2UoGb5im-q6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "wandb.login()"
      ],
      "metadata": {
        "id": "cRT4mmBem8fL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üëü Run an experiment\n",
        "1Ô∏è‚É£. **Start a new run** and pass in hyperparameters to track\n",
        "\n",
        "2Ô∏è‚É£. **Log metrics** from training or evaluation\n",
        "\n",
        "3Ô∏è‚É£. **Visualize results** in the dashboard\n",
        "\n",
        "Here, we will use a similar model as before, only with 2 layers instead of 1  "
      ],
      "metadata": {
        "id": "ZmkH9dbCnaq6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "wandb.init(\n",
        "    project=\"keras-intro\",\n",
        "    # (optional) set entity to specify your username or team name\n",
        "    # entity=\"my_team\",\n",
        "    config={\n",
        "        \"layer_1_neurons\": 10,\n",
        "        \"optimizer\": \"SGD\",\n",
        "        \"loss\": \"mean_squared_error\",\n",
        "        \"metric\": \"accuracy\",\n",
        "        \"epoch\": 10,\n",
        "        \"batch_size\": 8,\n",
        "    },\n",
        ")\n",
        "config = wandb.config\n",
        "\n",
        "# Get the data\n",
        "model = Sequential()\n",
        "dense_layer_1 = tf.keras.layers.Dense( config['layer_1_neurons'], input_shape=[784])\n",
        "model.add(dense_layer_1)\n",
        "dense_layer_2 = tf.keras.layers.Dense(10)\n",
        "model.add(dense_layer_2)\n",
        "\n",
        "\n",
        "model.compile(optimizer=config.optimizer, loss=config.loss, metrics=[config.metric])\n",
        "\n",
        "# Add WandbMetricsLogger to log metrics and WandbModelCheckpoint to log model checkpoints\n",
        "wandb_callbacks = [\n",
        "    WandbMetricsLogger(),\n",
        "    WandbModelCheckpoint(filepath=\"model_{epoch:02d}\"),\n",
        "]\n",
        "\n",
        "model.fit(\n",
        "    x=x_train.reshape(-1, 784),\n",
        "    y=y_train,\n",
        "    epochs=config.epoch,\n",
        "    batch_size=config.batch_size,\n",
        "    validation_data=(x_test, y_test),\n",
        "    callbacks=wandb_callbacks,\n",
        ")\n",
        "\n",
        "# Mark the run as finished\n",
        "wandb.finish()"
      ],
      "metadata": {
        "id": "Yv1j9nxMncHF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Experiment 2\n",
        "\n",
        "For this experiment, we will compare the performance of vaarying the number of neurons in the first layer"
      ],
      "metadata": {
        "id": "K553hOt_olVC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for neurons in [5,10,15,20,25]:\n",
        "  wandb.init(\n",
        "      project=    ''' TODO: SET AN APPROPIATE NAME FOR THIS RUN ''',\n",
        "      # (optional) set entity to specify your username or team name\n",
        "      # entity=\"my_team\",\n",
        "      config={\n",
        "          ''' CONFIGURE THIS MODEL SO THAT THE FIRST LAYER COTAINS\n",
        "            THE SPECIFIED NUMBER OF NEURONS '''\n",
        "      },\n",
        "  )\n",
        "  config = wandb.config\n",
        "\n",
        "  # Get the data\n",
        "  model = Sequential()\n",
        "  dense_layer_1 = tf.keras.layers.Dense( config['layer_1_neurons'], input_shape=[784])\n",
        "  model.add(dense_layer_1)\n",
        "  dense_layer_2 = tf.keras.layers.Dense(10)\n",
        "  model.add(dense_layer_2)\n",
        "\n",
        "\n",
        "  model.compile(optimizer=config.optimizer, loss=config.loss, metrics=[config.metric])\n",
        "\n",
        "  # Add WandbMetricsLogger to log metrics and WandbModelCheckpoint to log model checkpoints\n",
        "  wandb_callbacks = [\n",
        "      WandbMetricsLogger(),\n",
        "      WandbModelCheckpoint(filepath=\"model_{epoch:02d}\"),\n",
        "  ]\n",
        "\n",
        "  model.fit(\n",
        "      x=x_train.reshape(-1, 784),\n",
        "      y=y_train,\n",
        "      epochs=config.epoch,\n",
        "      batch_size=config.batch_size,\n",
        "      validation_data=(x_test, y_test),\n",
        "      callbacks=wandb_callbacks,\n",
        "  )\n",
        "\n",
        "  # Mark the run as finished\n",
        "  wandb.finish()"
      ],
      "metadata": {
        "id": "PFoEgs6UokhX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Experiment 3\n",
        "\n",
        "Choose a number of neurons for the first layer, and consider the impact of choosing different activation functions for the first layer"
      ],
      "metadata": {
        "id": "aydSyb_gpEcH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for act_funcs in [\"relu\", \"tanh\", \"sigmoid\"]:\n",
        "  wandb.init(\n",
        "      project=    ''' TODO: SET AN APPROPIATE NAME FOR THIS RUN ''',\n",
        "      # (optional) set entity to specify your username or team name\n",
        "      # entity=\"my_team\",\n",
        "      config={\n",
        "          ''' CONFIGURE THIS MODEL TO SPECIFY THE ACTIVATION FUNCTION FOR THIS RUN'''\n",
        "      },\n",
        "  )\n",
        "  config = wandb.config\n",
        "\n",
        "  # Get the data\n",
        "  model = Sequential()\n",
        "  dense_layer_1 = tf.keras.layers.Dense(''' TODO: SPECIFY THE NUMBER\n",
        "  OF NEURONS, AND CHOSEN ACTIVATION FUNCTION  ''',  input_shape=[784])\n",
        "  model.add(dense_layer_1)\n",
        "  dense_layer_2 = tf.keras.layers.Dense(10)\n",
        "  model.add(dense_layer_2)\n",
        "\n",
        "\n",
        "  model.compile(optimizer=config.optimizer, loss=config.loss, metrics=[config.metric])\n",
        "\n",
        "  # Add WandbMetricsLogger to log metrics and WandbModelCheckpoint to log model checkpoints\n",
        "  wandb_callbacks = [\n",
        "      WandbMetricsLogger(),\n",
        "      WandbModelCheckpoint(filepath=\"model_{epoch:02d}\"),\n",
        "  ]\n",
        "\n",
        "  model.fit(\n",
        "      x=x_train.reshape(-1, 784),\n",
        "      y=y_train,\n",
        "      epochs=config.epoch,\n",
        "      batch_size=config.batch_size,\n",
        "      validation_data=(x_test, y_test),\n",
        "      callbacks=wandb_callbacks,\n",
        "  )\n",
        "\n",
        "  # Mark the run as finished\n",
        "  wandb.finish()"
      ],
      "metadata": {
        "id": "2OXsF3H6pMA5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}