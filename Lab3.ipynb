{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<table align=\"center\">\n",
        "  <td align=\"center\"><a target=\"_blank\" href=\"https://colab.research.google.com/github/andrew-nash/CS6421-labs/blob/main/Lab3.ipynb\">\n",
        "        <img src=\"https://i.ibb.co/2P3SLwK/colab.png\"  style=\"padding-bottom:5px;\" />Run in Google Colab</a></td>\n",
        "  <td align=\"center\"><a target=\"_blank\" href=\"https://github.com/andrew-nash/CS6421-labs/blob/main/Lab3.ipynb\">\n",
        "        <img src=\"https://i.ibb.co/xfJbPmL/github.png\"  height=\"70px\" style=\"padding-bottom:5px;\"  />View Source on GitHub</a></td>\n",
        "</table>\n",
        "\n",
        "\n",
        "# Continued Model Optimiztion with TensorFlow\n",
        "\n",
        "In the last lab, we looked at a very basic end-to-end example of how models can be trained in TensorFlow. We saw how plugins such as TensorBoard and Weights and Biases are used to visualise the model training performance and compare the performance of selecting different hyper-parameters. In this Lab we will continue this process, but in more depth - we will consider more carefully the data processing requried, develop a simple (albeit non-trivial model) and perform  effective basic hyper-parameter tuning.\n",
        "\n",
        "Content based on: https://www.justintodata.com/hyperparameter-tuning-with-python-keras-guide/"
      ],
      "metadata": {
        "id": "ZKhQHmVVOMgt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "4XTXH_Fk07bM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# A Brief Sidebar on Numpy and Pandas\n",
        "\n",
        "We are going to be using a real-world dataset, taken from an older Kaggle competition (based on the Russian housing market from 2011-2015).\n",
        "\n",
        "In order to prepare this raw data in a format appropiate for use with Deep Learning models, we need to perform some *pre-processing*. There a number of potential tools for this job, in this particular case we will focus on NUMPY and PANDAS\n",
        "\n",
        "## What is Numpy?\n",
        "\n",
        "https://pub.towardsai.net/numpy-guide-super-simple-way-to-learn-it-in-10-minutes-d382ff45e215\n",
        "\n",
        "Short for Numerical Python, it is intended for use in high-performance computation on multi-dimensional arrays.\n",
        "\n",
        "### Creating numpy arrays\n",
        "\n",
        "Numpy arrays can be created very simply from python lists:"
      ],
      "metadata": {
        "id": "uTInNEhZztZs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "l = [1,2,3,4,5]\n",
        "a = np.array(l)\n",
        "print(a)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kT5mYQEeNMRK",
        "outputId": "06c34753-3a06-47df-f92e-285267e70203"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1 2 3 4 5]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "l = [[1,2,3,4,5],[6,7,8,9,10]]\n",
        "a = np.array(l)\n",
        "print(a)"
      ],
      "metadata": {
        "id": "VxT-zHTmNg5P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "l = [\n",
        "      [ [1,2,3],    [4,5,6] ],\n",
        "      [ [7,8,9],    [10,11,12] ],\n",
        "      [ [13,14,15], [16,17,18] ],\n",
        "      [ [19,20,21], [22,23,24] ]\n",
        "    ]\n",
        "a = np.array(l)\n",
        "print(a)"
      ],
      "metadata": {
        "id": "5Jd07n2oNn9q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Numpy arrays have some very useful attributes - particularly size, shape and dtpye\n",
        "\n",
        "1. *Size* tracks the number of scalar values contained within then array (and any sub-arrays)\n",
        "2. *Shape* contains the size of each dimension of the array - e.g. shape=(3,4) corresponds to a 3x4 matrix\n",
        "3. *Dtype* tracks the primitive type of the scalars in the array - such as np.float34, np.int32, np.int64 etc"
      ],
      "metadata": {
        "id": "5VHCZnAyNnaB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "l = [1,2,3,4,5]\n",
        "a = np.array(l)\n",
        "print(a.size, a.shape, a.dtype)"
      ],
      "metadata": {
        "id": "0fzAKtIvOUKl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before running the following code, try to predict the size, shape and dtype that will be printed"
      ],
      "metadata": {
        "id": "ccae1RL3O3sj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "l = [[1.2,2.5,3.5,4.5,5.5],[6.1,7.0,8.4184,9.1,10.14]]\n",
        "a = np.array(l)\n",
        "print(a.size, a.shape, a.dtype)"
      ],
      "metadata": {
        "id": "5lWXQEo-PD6t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "l = [\n",
        "      [ [1,2,3],    [4,5,6] ],\n",
        "      [ [7,8,9],    [10,11,12] ],\n",
        "      [ [13,14,15], [16,17,18] ],\n",
        "      [ [19,20,21], [22,23,24] ]\n",
        "    ]\n",
        "a = np.array(l)\n",
        "print(a.size, a.shape, a.dtype)"
      ],
      "metadata": {
        "id": "2EHdSi-OOcFy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Basic Array Operations"
      ],
      "metadata": {
        "id": "1CiYP-UpPLs1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "a = np.array([1, 2, 3])\n",
        "b = np.array([4, 5, 6])"
      ],
      "metadata": {
        "id": "tpv_NEhJPNr2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A huge beneift of numpy is that it allows simple arithmetic and linear algebraic operations to be applied simply to arrays.\n",
        "\n",
        "For example, basic matehmatical operations can be applied between two arrays (of the same shape). The operations will be applied element-by-element"
      ],
      "metadata": {
        "id": "Vt-59HACPSYd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "a+b"
      ],
      "metadata": {
        "id": "sFNhI5fpPlVJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a/b"
      ],
      "metadata": {
        "id": "iRWcT5Y5Ppd0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a*b"
      ],
      "metadata": {
        "id": "zIbc0Bx3Pr9u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Given two arrays that are **compatible** for matrix multiplication, this can be performed with:\n"
      ],
      "metadata": {
        "id": "aA2yBU_GPveA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "M = np.array([\n",
        "    [1,2,3],\n",
        "    [3,2,1]\n",
        "])\n",
        "\n",
        "v = np.array([2,4])"
      ],
      "metadata": {
        "id": "UPvzHRfgP4ty"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "v@M"
      ],
      "metadata": {
        "id": "CO1WBC0xP_yj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This also works to perform Tensor products for higher rank arrays."
      ],
      "metadata": {
        "id": "xipNe0MSQCYo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Other important numpy commands"
      ],
      "metadata": {
        "id": "pPXRcmF6QPAP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Creating arrays"
      ],
      "metadata": {
        "id": "IeXSgLu4Q3z5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# There are some other options for creating numpy arrays, such as:\n",
        "\n",
        "# create a 5x4 matrix of zeros\n",
        "zA = np.zeros(shape=(5,4), dtype=np.float64)\n",
        "\n",
        "# a 5x4 matrix of standard normal realizations\n",
        "nA = np.random.normal(0,1, (5,4))"
      ],
      "metadata": {
        "id": "sX3b4SI5QOcq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Indexing and slicing\n",
        "Numpy arrays can be indexed and sliced similarly to regular Python lists\n"
      ],
      "metadata": {
        "id": "M5TeQn6bQ5lv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "a = np.random.normal(0,1,(5,4))\n",
        "a"
      ],
      "metadata": {
        "id": "ogHjf__hRY9S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a[0,3]"
      ],
      "metadata": {
        "id": "tVLThvkIRkES"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a[1:,]"
      ],
      "metadata": {
        "id": "X-DsbuD5Rcrd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a[0,1:]"
      ],
      "metadata": {
        "id": "ajWDLBENRfa4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a = np.zeros(shape=(5,4,3))\n",
        "a = a+1"
      ],
      "metadata": {
        "id": "gcT4LWI6Q8BD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a[0,:,:]"
      ],
      "metadata": {
        "id": "YUM3xPxHQ78j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Reshaping\n",
        "\n",
        "Reshaping is an important operation in numpy - it allows us to change the shape of an array, while maintaining te same scalar elements. It allows us to increase and decrease the dimension of our data without changing values"
      ],
      "metadata": {
        "id": "uiB0H3V8Ru4Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "a = np.random.normal(0,1,(5,4))\n",
        "# rounding is possible in numpy, done here to make the array\n",
        "# easier to look at\n",
        "a = np.round(a, 2)\n",
        "a"
      ],
      "metadata": {
        "id": "8UFZXhtuR_kD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# convert this matrix into a Rank-3 tensor, with an outer dimension of 1\n",
        "a.reshape(1,5,4)"
      ],
      "metadata": {
        "id": "dGKlSZ7nSKiu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# convert this matrix into a Rank-1 tensor (a vector)\n",
        "a.reshape(20)"
      ],
      "metadata": {
        "id": "Bm4l79WASVaF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# the sizes of the dimenstions can also be swapped!\n",
        "# convert this matrix into a Rank-3 tensor, with an outer dimension of 1\n",
        "a.reshape(4,5)"
      ],
      "metadata": {
        "id": "8SbXEGhuSbjo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Important** - the last reshape operation shown here will **NOT** compute the transpose of a matrix or tensor"
      ],
      "metadata": {
        "id": "XjFc7AzxSl4L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# correctly computing the Transpose\n",
        "a.T"
      ],
      "metadata": {
        "id": "CMLzoNSZSk0k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Key takeaway: only use .reshape() for increasing and decreasing the rank of a tensor, not performing transposes!\n",
        "\n",
        "There is much more to numpy - vectorised operations, computing statistics such as max, min, mean, variance, etc. For now, the above knowledge will be more than sufficient for the purpose of this lab."
      ],
      "metadata": {
        "id": "fIfdzHGwS7sd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## What is Pandas\n",
        "\n",
        "https://pandas.pydata.org/docs/user_guide/10min.html\n",
        "\n",
        "*pandas is an open source, BSD-licensed library providing high-performance, easy-to-use data structures and data analysis tools for the Python programming language.*\n",
        "\n",
        "Pandas provides two types of classes for handling data Series and Dataframe. We will only look at Datafrane here"
      ],
      "metadata": {
        "id": "f0muTn0O132e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Download the data"
      ],
      "metadata": {
        "id": "X416S0Rm0y5w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://github.com/AdmiralWen/Sberbank-Russian-Housing-Market/raw/master/Data/train.csv"
      ],
      "metadata": {
        "id": "OLJzuJaPzsvN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load the data into pandas"
      ],
      "metadata": {
        "id": "alq8kA8v04dI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# reading a csv is simple in pandas!\n",
        "dataset = pd.read_csv(\"train.csv\")"
      ],
      "metadata": {
        "id": "SSElYIcU03vJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exploring the data"
      ],
      "metadata": {
        "id": "_UQ2JmKKLO7F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset"
      ],
      "metadata": {
        "id": "BaMBqbjJLQwN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset.columns"
      ],
      "metadata": {
        "id": "Oi3usVsDLU1t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#compiting summary statistics\n",
        "dataset.describe()"
      ],
      "metadata": {
        "id": "qOfKwZDULdr1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Basic Dataframe Selection\n",
        "\n",
        "Pandas allows us to select particular rows and/or columns out of the whole dataframe"
      ],
      "metadata": {
        "id": "MXgkw9vZ253H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#acccessing a particular column\n",
        "dataset[\"timestamp\"]"
      ],
      "metadata": {
        "id": "Bpcatkbu28qn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset.loc[2, \"timestamp\"]"
      ],
      "metadata": {
        "id": "t43VkDIh3Dwq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# accessing a particular row\n",
        "dataset.loc[2, :]"
      ],
      "metadata": {
        "id": "dhf41t8a3SwM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset[[\"id\", \"timestamp\"]]"
      ],
      "metadata": {
        "id": "6XKxQphf3Jf4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "It is also possible to perform **conditional** selection"
      ],
      "metadata": {
        "id": "a_s_yVCJMEGV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset['build_year']>2012"
      ],
      "metadata": {
        "id": "qFSgJpuSL11o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset[dataset['build_year']>2012]"
      ],
      "metadata": {
        "id": "0njcfNgYL-Bp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Pre-Processing"
      ],
      "metadata": {
        "id": "O7BGYuCFMKNN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "selected_columns = ['build_year','full_sq', 'life_sq', 'floor','product_type','area_m', \"price_doc\"]"
      ],
      "metadata": {
        "id": "PsTEBga52Hpj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sub_dataset = dataset[selected_columns]\n",
        "sub_dataset"
      ],
      "metadata": {
        "id": "ogduEQIe2lVv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data pre-processing issue #1 - missing data\n",
        "\n",
        "In this case we will drop rows where any data is missing. There are more sophisticated solutions to this issue, but these are outside the scope of this lab."
      ],
      "metadata": {
        "id": "MJ8-MCtl32DW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "clean_sub_dataset = sub_dataset.dropna()\n",
        "clean_sub_dataset"
      ],
      "metadata": {
        "id": "ULWngioE31Vc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this case, we have roughly halved the dataset - this is far from ideal considering how much useful data we have just thrown away!\n",
        "\n",
        "### Data Pre-Processing #2, convert Categorical data to numeric\n",
        "\n",
        "Keras will not be able to handle data such as `product_type` above, we will need to encode this to a numerical representation"
      ],
      "metadata": {
        "id": "2LgM8sVj4VrM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "set(clean_sub_dataset['product_type'])"
      ],
      "metadata": {
        "id": "QlYEiC8HgAXD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We see that there are two options for product_type - either Investment or OwnerOccupier. A simple option is to encode these as 0 and 1 respectively"
      ],
      "metadata": {
        "id": "0Dc3QQpHgm3X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "clean_sub_dataset=clean_sub_dataset.replace(\"OwnerOccupier\", 1)\n",
        "clean_sub_dataset=clean_sub_dataset.replace(\"Investment\", 0)"
      ],
      "metadata": {
        "id": "RGysElEBgym_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clean_sub_dataset"
      ],
      "metadata": {
        "id": "dMS7uRAQhxV5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Pre-processing #3, scaling\n",
        "\n",
        "In general neural netowrk optimisation works more effectively when data are closely distributed around 0. Why is this?\n",
        "\n",
        "First, we will export our pandas data to numpy"
      ],
      "metadata": {
        "id": "6zbQmb2Zg7uP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "np_data = clean_sub_dataset.to_numpy()\n",
        "np_data"
      ],
      "metadata": {
        "id": "kjgYiXD_hO5X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The y data (price), is the final column of the dataset"
      ],
      "metadata": {
        "id": "WYVGW7mGiug3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x_data = np_data[:,:-1]\n",
        "y_data = np_data[:,-1]"
      ],
      "metadata": {
        "id": "PPCBwEofiyQ-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Doing the scaling\n",
        "\n",
        " To scale the data, so that each column has values between 0 and 1 we perform a few steps.\n",
        "\n",
        " 1. Get the max and min of each column\n",
        " 2. Subtract the column-wise min from each value\n",
        " 3. Divide this subtracted value by (max-min)\n",
        "\n",
        " i.e.\n",
        "\n",
        " $\\displaystyle x_{scaled}=\\frac{x-x_{col min}}{x_{col max}-x_{col min}}$"
      ],
      "metadata": {
        "id": "z__yQlByi-zY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x_data.shape"
      ],
      "metadata": {
        "id": "F7In4vulj1sx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "col_max = np.max(x_data, axis=0)\n",
        "col_max.shape"
      ],
      "metadata": {
        "id": "-TKL4rNUi-JW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "col_min = np.min(x_data, axis=0)\n",
        "col_max.shape"
      ],
      "metadata": {
        "id": "kQI5RVdei8iK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_scaled = (x_data-col_min)/(col_max-col_min)"
      ],
      "metadata": {
        "id": "3drDzX1sj9yf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_scaled = (y_data-np.min(y_data))/(np.max(y_data)-np.min(y_data))"
      ],
      "metadata": {
        "id": "boc0l2ZGkHbP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The data is now ready for use!"
      ],
      "metadata": {
        "id": "L56pQqNhkS8e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Modelling this Data With Keras"
      ],
      "metadata": {
        "id": "qCnzw0A9rRL3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, partition the data into train and test splits"
      ],
      "metadata": {
        "id": "mfkoWZKdkixT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_frac = 0.7\n",
        "\n",
        "num_train_samples = int(train_frac*len(x_scaled))\n",
        "\n",
        "x_train = x_scaled[:num_train_samples,:]\n",
        "y_train = y_scaled[:num_train_samples]"
      ],
      "metadata": {
        "id": "3AiMqnmFkbuD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_test = x_scaled[num_train_samples:,:]\n",
        "y_test = y_scaled[num_train_samples:]"
      ],
      "metadata": {
        "id": "BPt5HLiIk0Vs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will now fit a model on this data, tuning the following hyper-parameters:\n",
        "\n",
        "1. The Number of layers used\n",
        "2. The number of neurons in each hidden layer\n",
        "3. The activation function used\n",
        "4. The number of training epochs and learning rate\n",
        "\n",
        "Finally, we will produce a model that should achieve good overall results!"
      ],
      "metadata": {
        "id": "ON-0VtQ6lZIL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Imports and installations"
      ],
      "metadata": {
        "id": "8KOlEjbkmZC8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf"
      ],
      "metadata": {
        "id": "pXL3V_kwmnu1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wandb -qU"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T8hTiGtqk4PC",
        "outputId": "429fd8fd-ca4a-459b-8187-28200293b23a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m21.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.4/196.4 kB\u001b[0m \u001b[31m20.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m257.7/257.7 kB\u001b[0m \u001b[31m30.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext tensorboard"
      ],
      "metadata": {
        "id": "j7KCahGkmdIc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import datetime\n",
        "log_dir = \"logs/\" + datetime.datetime.now().strftime(\"%Y%m%d\")"
      ],
      "metadata": {
        "id": "7O4Ti-sjmgWE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%tensorboard --logdir logs"
      ],
      "metadata": {
        "id": "bdmIjtjPmhJ3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import wandb\n",
        "from wandb.keras import WandbMetricsLogger, WandbModelCheckpoint\n",
        "wandb.login()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aK2ApL9Zm_fq",
        "outputId": "eeb4dd59-d42d-44b1-ef2c-59441235c2f5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 152
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Varying Number of Layers & Number of Neurons in Each Layer"
      ],
      "metadata": {
        "id": "CA4eFNw-myti"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for hidden_layers in [1,5,10,20,50]:\n",
        "  project_name = f\"varying-layers\"\n",
        "  run_name = f\"{hidden_layers}-layers\"\n",
        "\n",
        "  wandb.init(\n",
        "      project= project_name,\n",
        "      name = run_name ,\n",
        "      config={\n",
        "        \"layers\": hidden_layers,\n",
        "        \"optimizer\": \"SGD\",\n",
        "        \"loss\": \"mean_squared_error\",\n",
        "        \"epoch\": 10,\n",
        "        \"batch_size\": 8\n",
        "      },\n",
        "  )\n",
        "  config = wandb.config\n",
        "\n",
        "  #######\n",
        "  #### MODEL STARTS HERE\n",
        "  #######\n",
        "  model = tf.keras.models.Sequential()\n",
        "  dense_layer_1 = tf.keras.layers.Dense( 10, input_shape=[6])\n",
        "  model.add(dense_layer_1)\n",
        "  for l in range(config.layers):\n",
        "    dense_layer = tf.keras.layers.Dense(10)\n",
        "    model.add(dense_layer)\n",
        "\n",
        "  # output layer has 1 neuron\n",
        "  dense_layer = tf.keras.layers.Dense(1)\n",
        "  model.add(dense_layer)\n",
        "\n",
        "  model.compile(optimizer=config.optimizer, loss=config.loss, metrics=[])\n",
        "  #######\n",
        "  #### MODEL ENDS HERE\n",
        "  #######\n",
        "\n",
        "\n",
        "  # FORMATTING THE TENSORBOARD CALLBACK LOG DIRECTORY TO MAKE SEPARATE RUNS CLEARLY\n",
        "  tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir+'/'+project_name+'_'+run_name, update_freq=1, histogram_freq=1)\n",
        "  # Add WandbMetricsLogger to log metrics and WandbModelCheckpoint to log model checkpoints\n",
        "  wandb_callbacks = [\n",
        "      WandbMetricsLogger(),\n",
        "      WandbModelCheckpoint(filepath=\"model_{epoch:02d}\"),\n",
        "  ]\n",
        "\n",
        "  model.fit(\n",
        "      x=x_train,\n",
        "      y=y_train,\n",
        "      epochs=config.epoch,\n",
        "      batch_size=config.batch_size,\n",
        "      validation_data=(x_test, y_test),\n",
        "      callbacks=[wandb_callbacks, tensorboard_callback],\n",
        "      verbose=0\n",
        "  )\n",
        "\n",
        "  # Mark the run as finished\n",
        "  wandb.finish()"
      ],
      "metadata": {
        "id": "HRlGcAnDmyeA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exploring different activation functions\n",
        "\n",
        "Will activations such as `sigmoid` and `tanh` help?\n",
        "\n",
        "What about `leaky_relu`, `elu` ?"
      ],
      "metadata": {
        "id": "tZXRvcXot81f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for act in [\"relu\", \"elu\", \"leaky_relu\"]:\n",
        "  project_name = f\"varying-act-func\"\n",
        "  run_name = f\"{act}-func\"\n",
        "\n",
        "  wandb.init(\n",
        "      project= project_name,\n",
        "      name = run_name ,\n",
        "      config={\n",
        "        \"layers\": ''' TODO: DECIDE ON A NUMBER OF LAYERS''',\n",
        "        \"act\" : act,\n",
        "        \"optimizer\": \"SGD\",\n",
        "        \"loss\": \"mean_squared_error\",\n",
        "        \"epoch\": 10,\n",
        "        \"batch_size\": 8\n",
        "      },\n",
        "  )\n",
        "  config = wandb.config\n",
        "\n",
        "  #######\n",
        "  #### MODEL STARTS HERE\n",
        "  #######\n",
        "  model = tf.keras.models.Sequential()\n",
        "  dense_layer_1 = tf.keras.layers.Dense( 10, input_shape=[6],activation=config.act)\n",
        "  model.add(dense_layer_1)\n",
        "  for l in range(config.layers):\n",
        "    dense_layer = tf.keras.layers.Dense('''TODO: HOW MANY NEURONS PER LAYER''', activation=config.act)\n",
        "    model.add(dense_layer)\n",
        "\n",
        "  # output layer has 1 neuron\n",
        "  dense_layer = tf.keras.layers.Dense(1, activation=config.act)\n",
        "  model.add(dense_layer)\n",
        "\n",
        "  model.compile(optimizer=config.optimizer, loss=config.loss, metrics=[])\n",
        "  #######\n",
        "  #### MODEL ENDS HERE\n",
        "  #######\n",
        "\n",
        "\n",
        "  # FORMATTING THE TENSORBOARD CALLBACK LOG DIRECTORY TO MAKE SEPARATE RUNS CLEARLY\n",
        "  tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir+'/'+project_name+'_'+run_name, update_freq=1, histogram_freq=1)\n",
        "  # Add WandbMetricsLogger to log metrics and WandbModelCheckpoint to log model checkpoints\n",
        "  wandb_callbacks = [\n",
        "      WandbMetricsLogger(),\n",
        "      WandbModelCheckpoint(filepath=\"model_{epoch:02d}\"),\n",
        "  ]\n",
        "\n",
        "  model.fit(\n",
        "      x=x_train,\n",
        "      y=y_train,\n",
        "      epochs=config.epoch,\n",
        "      batch_size=config.batch_size,\n",
        "      validation_data=(x_test, y_test),\n",
        "      callbacks=[wandb_callbacks, tensorboard_callback],\n",
        "      verbose=0\n",
        "  )\n",
        "\n",
        "  # Mark the run as finished\n",
        "  wandb.finish()"
      ],
      "metadata": {
        "id": "e9NWCY8tqZhs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Regularization\n",
        "\n",
        "Regularization can be added to layers, to penalize high variance, quite simply.\n",
        "\n",
        "`tf.keras.layers.Dense(...,kernel_regularizer=tf.keras.regularizers.L2(l1=0.01), bias_regularizer=tf.keras.regularizers.L2(l1=0.01) )`\n",
        "\n",
        "If you suspect that your model could benefit from regularization, try adding these hyper-parameters and seeing if performance improves.\n"
      ],
      "metadata": {
        "id": "iCxQ9u0lGwrA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Varying Optimizer and Its Configuration\n",
        "\n",
        "So far we have used straightforward SGD - but what about more complex optimizers such as Adam and RMSProp ?\n",
        "\n",
        "Consider also the impact of changing the learning rate"
      ],
      "metadata": {
        "id": "7qmEIicFuCOG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "TODO: Adapt the code above to tune the optimizer, Learning rate, batch size and number of epochs of training\n",
        "\n",
        "BONUS: does adding Dropout ( tf.keras.layers.Dropout(p=0.2) ) to the model after each layer have any impact?\n",
        "'''"
      ],
      "metadata": {
        "id": "kcZoKuCeuU-d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Next Steps\n",
        "\n",
        "There are a few avenues to improve the performance of this model:\n",
        "\n",
        "1. Improve the dat preprocessing \\\\\n",
        "  a. We droppped all rows with missing data - is there a better way to impute this mssing data, to make better use of the other values in these rows that we are otherwise throwing away \\\\\n",
        "  b. Our system of encoding the categorical data was very simple - but is there an issue with it? \\\\\n",
        "2. Are there more effective strategies for optimising the choice of hyper-paremeters in the training process?\n",
        "\n",
        "\n",
        "Once you have settled on good sets of hyper-parameters, more exhaustive fine tuning can be performed with tools like the Keras Tuner (https://www.tensorflow.org/tutorials/keras/keras_tuner)."
      ],
      "metadata": {
        "id": "axEqPKk3lqr8"
      }
    }
  ]
}